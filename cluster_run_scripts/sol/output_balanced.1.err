2023-06-01 20:13:11.617205: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:13:11,676 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:13:11,773 INFO     [train_and_infer_api.py:122] finished running infer for 4266 values
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/44d44d7e-00f3-11ee-8161-e8ebd33a184c were not used when initializing TFBertForSequenceClassification: ['dropout_2735']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/44d44d7e-00f3-11ee-8161-e8ebd33a184c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:13:12.090995: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:13:12,346 INFO     [disk_cache.py:28] saving 4715 items to disk cache took 0.5539364814758301
2023-06-01 20:13:12,520 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 34035) in TrainAndInferHF
2023-06-01 20:13:12,536 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:13:12,543 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:13:12,543 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:13:12,549 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:13:12,551 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:13:12,555 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:13:12,555 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:13:12,556 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:13:12,557 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:13:12,557 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:13:12,576 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:13:12,596 INFO     [train_and_infer_api.py:122] finished running infer for 1077 values
2023-06-01 20:13:12,631 INFO     [train_and_dev_sets_selectors.py:27] using 450 for train using dataset trec_train and 776 for dev using dataset trec_dev
2023-06-01 20:13:12,631 INFO     [orchestrator_api.py:342] training a new model with {'false': 348, 'true': 102}
2023-06-01 20:13:12,632 INFO     [orchestrator_api.py:358] workspace imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM training a model for category 'LOC', model_metadata: {'train_counts': {'false': 348, 'true': 102}, 'dev_counts': {'false': 658, 'true': 118}}
2023-06-01 20:13:12,640 INFO     [train_and_infer_hf.py:96] Training hf model...
2023-06-01 20:13:12.648397: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-01 20:13:12,730 INFO     [disk_cache.py:28] saving 1077 items to disk cache took 0.1277012825012207
2023-06-01 20:13:12.731661: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:13:12,769 INFO     [experiment_runner.py:179] Evaluation on dataset: subjectivity_imbalanced_subjective_test, with AL: HARD_MINING, iteration: 2, repeat: 1, model (id: 426e5534-00f3-11ee-a5c3-e8ebd329a878) is: {'dataset': 'subjectivity_imbalanced_subjective_test', 'category': 'subjective', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 2, 'repeat id': 1, 'train positive count': 76, 'train negative count': 124, 'train total count': 200, 'BrierScore': 0.09440319119485763, 'LogLoss': 0.1615364041265994, 'confECE': 0.042581963639717356, 'cwECE': 0.0565274741791168, 'Acc': 93.87186629526462, 'MSE': nan, 'average_score': 0.9445573713564718, 'accuracy': 0.9387186629526463, 'precision': 0.6381578947368421, 'recall': 0.8981481481481481, 'f1': 0.7461538461538462, 'support': 108, 'tp': 97, 'fp': 55, 'tn': 914, 'fn': 11, 'diversity': 0.04550788477797899, 'representativeness': 0.206353245407132}	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:13:12,774 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 3, repeat num: 1	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:13:12,835 INFO     [active_learning_api.py:59] Got 3719 unlabeled elements for active learning
2023-06-01 20:13:12,975 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3719 values (cache size 75190) in TrainAndInferHF
2023-06-01 20:13:12,978 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:13:13.367279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/426e5534-00f3-11ee-a5c3-e8ebd329a878 were not used when initializing TFBertForSequenceClassification: ['dropout_1937']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/426e5534-00f3-11ee-a5c3-e8ebd329a878.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:13:14.509497: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:13:15.189894: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:13:15.190475: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:13:15.441967: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)
usage: ActiveTrainer [-h] [--dataset DATASET] [--n_mc N_MC]
                     [--query_step QUERY_STEP] [--nseed NSEED]
                     [--nquery_steps NQUERY_STEPS]
ActiveTrainer: error: unrecognized arguments: --label_id=True
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:13:23,693 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:13:23,721 INFO     [train_and_infer_api.py:122] finished running infer for 1043 values
2023-06-01 20:13:23,855 INFO     [disk_cache.py:28] saving 1039 items to disk cache took 0.1256113052368164
2023-06-01 20:13:23,907 INFO     [experiment_runner.py:179] Evaluation on dataset: cola_test, with AL: HARD_MINING, iteration: 9, repeat: 1, model (id: 44d44d7e-00f3-11ee-8161-e8ebd33a184c) is: {'dataset': 'cola_test', 'category': '1', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 9, 'repeat id': 1, 'train positive count': 277, 'train negative count': 273, 'train total count': 550, 'BrierScore': 0.3816166427030894, 'LogLoss': 0.6766650707568647, 'confECE': 0.15792982804589403, 'cwECE': 0.15792982956110604, 'Acc': 77.37296260786194, 'MSE': nan, 'average_score': 0.9316594561245846, 'accuracy': 0.7737296260786194, 'precision': 0.790167865707434, 'recall': 0.9152777777777777, 'f1': 0.8481338481338481, 'support': 720, 'tp': 659, 'fp': 175, 'tn': 148, 'fn': 61, 'diversity': 0.09560783975198811, 'representativeness': 0.22957679018333305}	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
2023-06-01 20:13:23,926 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 10, repeat num: 1	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
2023-06-01 20:13:24,065 INFO     [active_learning_api.py:59] Got 7027 unlabeled elements for active learning
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:13:24,831 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:13:24,852 INFO     [train_and_infer_api.py:122] finished running infer for 1193 values
2023-06-01 20:13:25,016 INFO     [disk_cache.py:28] saving 1193 items to disk cache took 0.15868210792541504
2023-06-01 20:13:25,069 INFO     [experiment_runner.py:179] Evaluation on dataset: polarity_imbalanced_positive_test, with AL: RANDOM, iteration: 12, repeat: 1, model (id: 48fcd092-00f3-11ee-b509-e8ebd329a958) is: {'dataset': 'polarity_imbalanced_positive_test', 'category': 'positive', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 12, 'repeat id': 1, 'train positive count': 97, 'train negative count': 603, 'train total count': 700, 'BrierScore': 0.17816732496173288, 'LogLoss': 0.3325650592868901, 'confECE': 0.0676877700982043, 'cwECE': 0.07659717085587808, 'Acc': 89.35456831517183, 'MSE': nan, 'average_score': 0.961233453398878, 'accuracy': 0.8935456831517183, 'precision': 0.474025974025974, 'recall': 0.6134453781512605, 'f1': 0.5347985347985348, 'support': 119, 'tp': 73, 'fp': 81, 'tn': 993, 'fn': 46, 'diversity': 0.24855958068096648, 'representativeness': 0.41929011161295265}	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM
2023-06-01 20:13:25,073 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 13, repeat num: 1	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM
2023-06-01 20:13:25,214 INFO     [active_learning_api.py:59] Got 3442 unlabeled elements for active learning
2023-06-01 20:13:26,426 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: polarity_imbalanced_positive_train and category: positive.	runtime: 1.3524856567382812	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM
2023-06-01 20:13:26,658 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3442 values (cache size 60713) in TrainAndInferHF
2023-06-01 20:13:26,661 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:13:27,183 INFO     [train_and_infer_api.py:111] 15 already in cache, running infer for 7012 values (cache size 190592) in TrainAndInferHF
2023-06-01 20:13:27,191 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/48fcd092-00f3-11ee-b509-e8ebd329a958 were not used when initializing TFBertForSequenceClassification: ['dropout_1405']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/48fcd092-00f3-11ee-b509-e8ebd329a958.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/44d44d7e-00f3-11ee-8161-e8ebd33a184c were not used when initializing TFBertForSequenceClassification: ['dropout_2735']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/44d44d7e-00f3-11ee-8161-e8ebd33a184c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:13:29.080848: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:13:29,249 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:13:29,250 INFO     [orchestrator_api.py:376] new model id is 5030af32-00f3-11ee-b1d5-e8ebd305f284
2023-06-01 20:13:29,411 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 2000 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:13:29,413 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:13:29.512950: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/5030af32-00f3-11ee-b1d5-e8ebd305f284 were not used when initializing TFBertForSequenceClassification: ['dropout_2963']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/5030af32-00f3-11ee-b1d5-e8ebd305f284.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:13:33.591147: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:13:43.941362: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:13:55,758 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:13:55,798 INFO     [train_and_infer_api.py:122] finished running infer for 2000 values
2023-06-01 20:13:56,068 INFO     [disk_cache.py:28] saving 2000 items to disk cache took 0.2514309883117676
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:13:56,090 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:13:56,091 INFO     [orchestrator_api.py:376] new model id is 55ecdef0-00f3-11ee-9023-e8ebd329a838
2023-06-01 20:13:56,137 INFO     [experiment_runner.py:179] Evaluation on dataset: subjectivity_test, with AL: HARD_MINING, iteration: 11, repeat: 1, model (id: 5030af32-00f3-11ee-b1d5-e8ebd305f284) is: {'dataset': 'subjectivity_test', 'category': 'objective', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 11, 'repeat id': 1, 'train positive count': 332, 'train negative count': 318, 'train total count': 650, 'BrierScore': 0.06612897302233182, 'LogLoss': 0.13734975107958985, 'confECE': 0.020587636929001833, 'cwECE': 0.0185023957187077, 'Acc': 96.1, 'MSE': nan, 'average_score': 0.9794955508708953, 'accuracy': 0.961, 'precision': 0.9550561797752809, 'recall': 0.9649122807017544, 'f1': 0.959958932238193, 'support': 969, 'tp': 935, 'fp': 44, 'tn': 987, 'fn': 34, 'diversity': 0.054689629531176995, 'representativeness': 0.15621646772883932}	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:13:56,140 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 12, repeat num: 1	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:13:56,230 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3000 values (cache size 71240) in TrainAndInferHF
2023-06-01 20:13:56,232 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:13:56,300 INFO     [active_learning_api.py:59] Got 6350 unlabeled elements for active learning
2023-06-01 20:13:56,569 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 6350 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:13:56,575 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/5030af32-00f3-11ee-b1d5-e8ebd305f284 were not used when initializing TFBertForSequenceClassification: ['dropout_2963']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/5030af32-00f3-11ee-b1d5-e8ebd305f284.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/55ecdef0-00f3-11ee-9023-e8ebd329a838 were not used when initializing TFBertForSequenceClassification: ['dropout_493']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/55ecdef0-00f3-11ee-9023-e8ebd329a838.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:13:57,967 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:13:58,043 INFO     [train_and_infer_api.py:122] finished running infer for 3719 values
2023-06-01 20:13:58,620 INFO     [disk_cache.py:28] saving 4796 items to disk cache took 0.5643901824951172
2023-06-01 20:13:58,673 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: subjectivity_imbalanced_subjective_train and category: subjective.	runtime: 45.89896512031555	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:13:59.826014: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:14:00,223 INFO     [train_and_infer_api.py:111] 3719 already in cache, running infer for 0 values (cache size 78909) in TrainAndInferHF
2023-06-01 20:14:00,347 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 78909) in TrainAndInferHF
2023-06-01 20:14:00,356 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:00,358 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:14:00,358 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:00,362 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:14:00,362 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:00,365 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:00,366 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:14:00,366 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:00,367 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:14:00,367 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:00,425 INFO     [train_and_dev_sets_selectors.py:27] using 250 for train using dataset subjectivity_imbalanced_subjective_train and 560 for dev using dataset subjectivity_imbalanced_subjective_dev
2023-06-01 20:14:00,425 INFO     [orchestrator_api.py:342] training a new model with {'false': 167, 'true': 83}
2023-06-01 20:14:00,427 INFO     [orchestrator_api.py:358] workspace imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING training a model for category 'subjective', model_metadata: {'train_counts': {'false': 167, 'true': 83}, 'dev_counts': {'false': 504, 'true': 56}}
2023-06-01 20:14:00,432 INFO     [train_and_infer_hf.py:96] Training hf model...
2023-06-01 20:14:00.732776: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:14:01.861103: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:14:01.861662: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:14:05,682 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:14:05,684 INFO     [orchestrator_api.py:376] new model id is 68904d80-00f3-11ee-a89c-e8ebd335ce06
2023-06-01 20:14:05,777 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 500 values (cache size 34035) in TrainAndInferHF
2023-06-01 20:14:05,778 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/68904d80-00f3-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_835']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/68904d80-00f3-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:14:06,805 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:14:06.821500: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:14:06,870 INFO     [train_and_infer_api.py:122] finished running infer for 3442 values
2023-06-01 20:14:07,417 INFO     [disk_cache.py:28] saving 4635 items to disk cache took 0.5348289012908936
2023-06-01 20:14:07,589 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 64155) in TrainAndInferHF
2023-06-01 20:14:07,615 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:07,616 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:14:07,617 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:07,620 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:14:07,620 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:07,623 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:07,624 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:14:07,624 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:07,624 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:14:07,625 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:07,695 INFO     [train_and_dev_sets_selectors.py:27] using 750 for train using dataset polarity_imbalanced_positive_train and 588 for dev using dataset polarity_imbalanced_positive_dev
2023-06-01 20:14:07,695 INFO     [orchestrator_api.py:342] training a new model with {'false': 644, 'true': 106}
2023-06-01 20:14:07,696 INFO     [orchestrator_api.py:358] workspace imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM training a model for category 'positive', model_metadata: {'train_counts': {'false': 644, 'true': 106}, 'dev_counts': {'false': 529, 'true': 59}}
2023-06-01 20:14:07,702 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:14:09.477307: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:14:09.477858: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:14:12,423 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:14:12,434 INFO     [train_and_infer_api.py:122] finished running infer for 500 values
2023-06-01 20:14:12,496 INFO     [disk_cache.py:28] saving 500 items to disk cache took 0.05806732177734375
2023-06-01 20:14:12,546 INFO     [experiment_runner.py:179] Evaluation on dataset: trec_test, with AL: RANDOM, iteration: 7, repeat: 1, model (id: 68904d80-00f3-11ee-a89c-e8ebd335ce06) is: {'dataset': 'trec_test', 'category': 'LOC', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 7, 'repeat id': 1, 'train positive count': 102, 'train negative count': 348, 'train total count': 450, 'BrierScore': 0.04052962263128843, 'LogLoss': 0.09627974845390554, 'confECE': 0.003064103722645212, 'cwECE': 0.025940919688902803, 'Acc': 97.8, 'MSE': nan, 'average_score': 0.9782745394706726, 'accuracy': 0.978, 'precision': 0.972972972972973, 'recall': 0.8888888888888888, 'f1': 0.9290322580645162, 'support': 81, 'tp': 72, 'fp': 2, 'tn': 417, 'fn': 9, 'diversity': 0.29154320762181096, 'representativeness': 0.5502972413578794}	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:14:12,559 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 8, repeat num: 1	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:14:12,662 INFO     [active_learning_api.py:59] Got 4168 unlabeled elements for active learning
2023-06-01 20:14:12,741 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: trec_train and category: LOC.	runtime: 0.1810312271118164	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:14:12,940 INFO     [train_and_infer_api.py:111] 8 already in cache, running infer for 4216 values (cache size 34535) in TrainAndInferHF
2023-06-01 20:14:12,944 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/68904d80-00f3-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_835']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/68904d80-00f3-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:14:15.937321: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:14:21,038 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:14:21,163 INFO     [train_and_infer_api.py:122] finished running infer for 6963 values
2023-06-01 20:14:22,272 INFO     [disk_cache.py:28] saving 9096 items to disk cache took 1.0461552143096924
2023-06-01 20:14:22,406 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: polarity_train and category: positive.	runtime: 83.47461342811584	workspace: balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING
2023-06-01 20:14:25,148 INFO     [train_and_infer_api.py:111] 6963 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:14:25,378 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:14:25,398 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:25,400 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:14:25,401 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:25,408 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:14:25,408 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:25,416 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:25,416 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:14:25,417 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:25,418 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:14:25,418 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:25,516 INFO     [train_and_dev_sets_selectors.py:27] using 550 for train using dataset polarity_train and 1066 for dev using dataset polarity_dev
2023-06-01 20:14:25,516 INFO     [orchestrator_api.py:342] training a new model with {'false': 303, 'true': 247}
2023-06-01 20:14:25,517 INFO     [orchestrator_api.py:358] workspace balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING training a model for category 'positive', model_metadata: {'train_counts': {'false': 303, 'true': 247}, 'dev_counts': {'true': 537, 'false': 529}}
2023-06-01 20:14:25,526 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:14:27.212521: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:14:27.213103: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:14:28.389313: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:14:34,034 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:14:34,091 INFO     [train_and_infer_api.py:122] finished running infer for 3000 values
2023-06-01 20:14:34,502 INFO     [disk_cache.py:28] saving 2999 items to disk cache took 0.4011828899383545
2023-06-01 20:14:34,577 INFO     [experiment_runner.py:179] Evaluation on dataset: wiki_attack_test, with AL: RANDOM, iteration: 4, repeat: 1, model (id: 55ecdef0-00f3-11ee-9023-e8ebd329a838) is: {'dataset': 'wiki_attack_test', 'category': 'True', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 4, 'repeat id': 1, 'train positive count': 71, 'train negative count': 229, 'train total count': 300, 'BrierScore': 0.11886206243228602, 'LogLoss': 0.21426379526876985, 'confECE': 0.018483504650931147, 'cwECE': 0.02430972342286259, 'Acc': 92.5, 'MSE': nan, 'average_score': 0.9394422115882238, 'accuracy': 0.925, 'precision': 0.7642276422764228, 'recall': 0.5295774647887324, 'f1': 0.6256239600665556, 'support': 355, 'tp': 188, 'fp': 58, 'tn': 2587, 'fn': 167, 'diversity': 0.2063955935760682, 'representativeness': 0.41740707674133726}	workspace: imbalanced_wiki_attack_calib_nseed_100_step_50_nactive_15_MC_10-wiki_attack-True-HFBERT-1-RANDOM
2023-06-01 20:14:34,580 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 5, repeat num: 1	workspace: imbalanced_wiki_attack_calib_nseed_100_step_50_nactive_15_MC_10-wiki_attack-True-HFBERT-1-RANDOM
2023-06-01 20:14:34,740 INFO     [active_learning_api.py:59] Got 14691 unlabeled elements for active learning
2023-06-01 20:14:34,810 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: wiki_attack_train and category: True.	runtime: 0.22985482215881348	workspace: imbalanced_wiki_attack_calib_nseed_100_step_50_nactive_15_MC_10-wiki_attack-True-HFBERT-1-RANDOM
2023-06-01 20:14:36,892 INFO     [train_and_infer_api.py:111] 9 already in cache, running infer for 14691 values (cache size 74239) in TrainAndInferHF
2023-06-01 20:14:36,906 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:14:37.483571: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/55ecdef0-00f3-11ee-9023-e8ebd329a838 were not used when initializing TFBertForSequenceClassification: ['dropout_493']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/55ecdef0-00f3-11ee-9023-e8ebd329a838.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:14:42,597 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:14:42,823 INFO     [train_and_infer_api.py:122] finished running infer for 12163 values
2023-06-01 20:14:44,451 INFO     [disk_cache.py:28] saving 14660 items to disk cache took 1.5950806140899658
2023-06-01 20:14:44,833 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 103670) in TrainAndInferHF
2023-06-01 20:14:44,851 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:44,854 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:14:44,855 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:44,865 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:14:44,865 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:44,875 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:44,876 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:14:44,877 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:44,877 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:14:44,878 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:45,024 INFO     [train_and_dev_sets_selectors.py:27] using 450 for train using dataset ag_news_imbalanced_1_train and 2455 for dev using dataset ag_news_imbalanced_1_dev
2023-06-01 20:14:45,024 INFO     [orchestrator_api.py:342] training a new model with {'false': 356, 'true': 94}
2023-06-01 20:14:45,025 INFO     [orchestrator_api.py:358] workspace imbalanced_ag_news_imbalanced_1_calib_nseed_100_step_50_nactive_15_MC_10-ag_news_imbalanced_1-1-HFBERT-1-RANDOM training a model for category '1', model_metadata: {'train_counts': {'false': 356, 'true': 94}, 'dev_counts': {'false': 2209, 'true': 246}}
2023-06-01 20:14:45,041 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:14:47,263 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:14:47,264 INFO     [orchestrator_api.py:376] new model id is 850cd35c-00f3-11ee-a5c3-e8ebd329a878
2023-06-01 20:14:47,316 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1077 values (cache size 78909) in TrainAndInferHF
2023-06-01 20:14:47,317 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:14:47,400 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:14:47,546 INFO     [train_and_infer_api.py:122] finished running infer for 7012 values
2023-06-01 20:14:47.702430: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:14:47.702911: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/850cd35c-00f3-11ee-a5c3-e8ebd329a878 were not used when initializing TFBertForSequenceClassification: ['dropout_2051']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/850cd35c-00f3-11ee-a5c3-e8ebd329a878.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:14:48,470 INFO     [disk_cache.py:28] saving 8051 items to disk cache took 0.9017670154571533
2023-06-01 20:14:48,609 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: cola_train and category: 1.	runtime: 84.6830506324768	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
2023-06-01 20:14:48,748 INFO     [train_and_infer_api.py:111] 7042 already in cache, running infer for 0 values (cache size 197604) in TrainAndInferHF
2023-06-01 20:14:48.763078: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:14:48,987 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 197604) in TrainAndInferHF
2023-06-01 20:14:49,008 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:49,010 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:14:49,011 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:49,017 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:14:49,018 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:49,028 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:49,028 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:14:49,029 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:49,029 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:14:49,030 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:14:49,128 INFO     [train_and_dev_sets_selectors.py:27] using 600 for train using dataset cola_train and 959 for dev using dataset cola_dev
2023-06-01 20:14:49,128 INFO     [orchestrator_api.py:342] training a new model with {'false': 293, 'true': 307}
2023-06-01 20:14:49,129 INFO     [orchestrator_api.py:358] workspace balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING training a model for category '1', model_metadata: {'train_counts': {'false': 293, 'true': 307}, 'dev_counts': {'true': 673, 'false': 286}}
2023-06-01 20:14:49,137 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:14:50.662339: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:14:50.662952: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:14:53.598673: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:14:54.877249: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:14:59,949 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:14:59,950 INFO     [orchestrator_api.py:376] new model id is 89622240-00f3-11ee-b509-e8ebd329a958
2023-06-01 20:15:00,113 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1193 values (cache size 64155) in TrainAndInferHF
2023-06-01 20:15:00,115 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:15:00,777 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:15:00,796 INFO     [train_and_infer_api.py:122] finished running infer for 1077 values
2023-06-01 20:15:00,956 INFO     [disk_cache.py:28] saving 1077 items to disk cache took 0.15444302558898926
2023-06-01 20:15:00,995 INFO     [experiment_runner.py:179] Evaluation on dataset: subjectivity_imbalanced_subjective_test, with AL: HARD_MINING, iteration: 3, repeat: 1, model (id: 850cd35c-00f3-11ee-a5c3-e8ebd329a878) is: {'dataset': 'subjectivity_imbalanced_subjective_test', 'category': 'subjective', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 3, 'repeat id': 1, 'train positive count': 83, 'train negative count': 167, 'train total count': 250, 'BrierScore': 0.10150061254952628, 'LogLoss': 0.16439537713773641, 'confECE': 0.0371207833356338, 'cwECE': 0.062331060314258724, 'Acc': 92.47910863509749, 'MSE': nan, 'average_score': 0.9366172138694934, 'accuracy': 0.924791086350975, 'precision': 0.5780346820809249, 'recall': 0.9259259259259259, 'f1': 0.7117437722419929, 'support': 108, 'tp': 100, 'fp': 73, 'tn': 896, 'fn': 8, 'diversity': 0.07473225704506456, 'representativeness': 0.14916133012060753}	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:15:01,029 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 4, repeat num: 1	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:15:01,094 INFO     [active_learning_api.py:59] Got 3669 unlabeled elements for active learning
2023-06-01 20:15:01,239 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3669 values (cache size 79986) in TrainAndInferHF
2023-06-01 20:15:01,242 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/89622240-00f3-11ee-b509-e8ebd329a958 were not used when initializing TFBertForSequenceClassification: ['dropout_1519']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/89622240-00f3-11ee-b509-e8ebd329a958.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:15:01.819547: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:15:02,336 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:15:02,426 INFO     [train_and_infer_api.py:122] finished running infer for 4216 values
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/850cd35c-00f3-11ee-a5c3-e8ebd329a878 were not used when initializing TFBertForSequenceClassification: ['dropout_2051']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/850cd35c-00f3-11ee-a5c3-e8ebd329a878.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:15:02,965 INFO     [disk_cache.py:28] saving 4665 items to disk cache took 0.5228581428527832
2023-06-01 20:15:03,137 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 38700) in TrainAndInferHF
2023-06-01 20:15:03,155 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:03,156 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:15:03,156 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:03,160 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:15:03,161 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:03,171 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:03,171 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:15:03,171 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:03,172 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:15:03,172 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:03,242 INFO     [train_and_dev_sets_selectors.py:27] using 500 for train using dataset trec_train and 776 for dev using dataset trec_dev
2023-06-01 20:15:03,243 INFO     [orchestrator_api.py:342] training a new model with {'false': 393, 'true': 107}
2023-06-01 20:15:03,243 INFO     [orchestrator_api.py:358] workspace imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM training a model for category 'LOC', model_metadata: {'train_counts': {'false': 393, 'true': 107}, 'dev_counts': {'false': 658, 'true': 118}}
2023-06-01 20:15:03,249 INFO     [train_and_infer_hf.py:96] Training hf model...
2023-06-01 20:15:03.862320: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:15:04.749101: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:15:04.749638: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:15:09,820 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:15:09,914 INFO     [train_and_infer_api.py:122] finished running infer for 6350 values
2023-06-01 20:15:10,910 INFO     [disk_cache.py:28] saving 8350 items to disk cache took 0.9430041313171387
2023-06-01 20:15:11,081 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: subjectivity_train and category: objective.	runtime: 74.94021224975586	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:15:11,242 INFO     [train_and_infer_api.py:111] 6350 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:15:11,481 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:15:11,507 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:11,509 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:15:11,510 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:11,516 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:15:11,517 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:11,526 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:11,526 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:15:11,527 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:11,527 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:15:11,527 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:11,647 INFO     [train_and_dev_sets_selectors.py:27] using 700 for train using dataset subjectivity_train and 1000 for dev using dataset subjectivity_dev
2023-06-01 20:15:11,647 INFO     [orchestrator_api.py:342] training a new model with {'false': 351, 'true': 349}
2023-06-01 20:15:11,649 INFO     [orchestrator_api.py:358] workspace balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING training a model for category 'objective', model_metadata: {'train_counts': {'false': 351, 'true': 349}, 'dev_counts': {'true': 504, 'false': 496}}
2023-06-01 20:15:11,657 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:15:13.337452: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:15:13.337990: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:15:15,119 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:15:15,141 INFO     [train_and_infer_api.py:122] finished running infer for 1193 values
2023-06-01 20:15:15,312 INFO     [disk_cache.py:28] saving 1193 items to disk cache took 0.16242432594299316
2023-06-01 20:15:15,364 INFO     [experiment_runner.py:179] Evaluation on dataset: polarity_imbalanced_positive_test, with AL: RANDOM, iteration: 13, repeat: 1, model (id: 89622240-00f3-11ee-b509-e8ebd329a958) is: {'dataset': 'polarity_imbalanced_positive_test', 'category': 'positive', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 13, 'repeat id': 1, 'train positive count': 106, 'train negative count': 644, 'train total count': 750, 'BrierScore': 0.13159223337054493, 'LogLoss': 0.26441230931286763, 'confECE': 0.05524025709126223, 'cwECE': 0.04445582531331684, 'Acc': 92.5398155909472, 'MSE': nan, 'average_score': 0.9769295434040476, 'accuracy': 0.9253981559094719, 'precision': 0.6875, 'recall': 0.46218487394957986, 'f1': 0.5527638190954773, 'support': 119, 'tp': 55, 'fp': 25, 'tn': 1049, 'fn': 64, 'diversity': 0.22324372855491906, 'representativeness': 0.34717695755300865}	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM
2023-06-01 20:15:15,367 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 14, repeat num: 1	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM
2023-06-01 20:15:15.511394: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:15:15,520 INFO     [active_learning_api.py:59] Got 3392 unlabeled elements for active learning
2023-06-01 20:15:15,647 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: polarity_imbalanced_positive_train and category: positive.	runtime: 0.2799861431121826	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM
2023-06-01 20:15:15,894 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3392 values (cache size 65348) in TrainAndInferHF
2023-06-01 20:15:15,897 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:15:18.206618: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/89622240-00f3-11ee-b509-e8ebd329a958 were not used when initializing TFBertForSequenceClassification: ['dropout_1519']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/89622240-00f3-11ee-b509-e8ebd329a958.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:15:19.596970: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:15:22,267 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:15:22,268 INFO     [orchestrator_api.py:376] new model id is 9401dd4e-00f3-11ee-9f77-e8ebd339fe3c
2023-06-01 20:15:22,412 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 2133 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:15:22,413 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/9401dd4e-00f3-11ee-9f77-e8ebd339fe3c were not used when initializing TFBertForSequenceClassification: ['dropout_2735']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/9401dd4e-00f3-11ee-9f77-e8ebd339fe3c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:15:24.188662: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:15:32.305971: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:15:41.486303: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:15:43,479 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:15:43,480 INFO     [orchestrator_api.py:376] new model id is a214b9ec-00f3-11ee-8161-e8ebd33a184c
2023-06-01 20:15:43,640 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1043 values (cache size 197604) in TrainAndInferHF
2023-06-01 20:15:43,641 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:15:44,568 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:15:44,631 INFO     [train_and_infer_api.py:122] finished running infer for 3669 values
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/a214b9ec-00f3-11ee-8161-e8ebd33a184c were not used when initializing TFBertForSequenceClassification: ['dropout_2849']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/a214b9ec-00f3-11ee-8161-e8ebd33a184c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:15:44.895853: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:15:45,179 INFO     [disk_cache.py:28] saving 4746 items to disk cache took 0.53182053565979
2023-06-01 20:15:45,243 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: subjectivity_imbalanced_subjective_train and category: subjective.	runtime: 44.214149951934814	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:15:45,311 INFO     [train_and_infer_api.py:111] 3669 already in cache, running infer for 0 values (cache size 83655) in TrainAndInferHF
2023-06-01 20:15:45,428 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 83655) in TrainAndInferHF
2023-06-01 20:15:45,439 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:45,440 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:15:45,441 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:45,445 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:15:45,445 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:45,449 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:45,449 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:15:45,450 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:45,450 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:15:45,451 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:45,503 INFO     [train_and_dev_sets_selectors.py:27] using 300 for train using dataset subjectivity_imbalanced_subjective_train and 560 for dev using dataset subjectivity_imbalanced_subjective_dev
2023-06-01 20:15:45,503 INFO     [orchestrator_api.py:342] training a new model with {'false': 211, 'true': 89}
2023-06-01 20:15:45,504 INFO     [orchestrator_api.py:358] workspace imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING training a model for category 'subjective', model_metadata: {'train_counts': {'false': 211, 'true': 89}, 'dev_counts': {'false': 504, 'true': 56}}
2023-06-01 20:15:45,508 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:15:47.002501: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:15:47.003038: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:15:47,613 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:15:47,614 INFO     [orchestrator_api.py:376] new model id is 9fa39e80-00f3-11ee-8ba6-1070fd5ed7e8
2023-06-01 20:15:47,762 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 2513 values (cache size 103670) in TrainAndInferHF
2023-06-01 20:15:47,763 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/9fa39e80-00f3-11ee-8ba6-1070fd5ed7e8 were not used when initializing TFBertForSequenceClassification: ['dropout_835']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/9fa39e80-00f3-11ee-8ba6-1070fd5ed7e8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:15:49.943272: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:15:50,412 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:15:50,446 INFO     [train_and_infer_api.py:122] finished running infer for 2133 values
2023-06-01 20:15:50,712 INFO     [disk_cache.py:28] saving 2133 items to disk cache took 0.2317821979522705
2023-06-01 20:15:50,778 INFO     [experiment_runner.py:179] Evaluation on dataset: polarity_test, with AL: HARD_MINING, iteration: 9, repeat: 1, model (id: 9401dd4e-00f3-11ee-9f77-e8ebd339fe3c) is: {'dataset': 'polarity_test', 'category': 'positive', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 9, 'repeat id': 1, 'train positive count': 247, 'train negative count': 303, 'train total count': 550, 'BrierScore': 0.2999114559468924, 'LogLoss': 0.5301520820684799, 'confECE': 0.12162845843085275, 'cwECE': 0.12370523377210321, 'Acc': 81.62212845757149, 'MSE': nan, 'average_score': 0.9378497430655636, 'accuracy': 0.816221284575715, 'precision': 0.8285714285714286, 'recall': 0.7941454202077431, 'f1': 0.81099324975892, 'support': 1059, 'tp': 841, 'fp': 174, 'tn': 900, 'fn': 218, 'diversity': 0.05296051509352115, 'representativeness': 0.18701581346543156}	workspace: balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING
2023-06-01 20:15:50,780 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 10, repeat num: 1	workspace: balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING
2023-06-01 20:15:50,923 INFO     [active_learning_api.py:59] Got 6913 unlabeled elements for active learning
2023-06-01 20:15:51,202 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 6913 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:15:51,208 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/9401dd4e-00f3-11ee-9f77-e8ebd339fe3c were not used when initializing TFBertForSequenceClassification: ['dropout_2735']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/9401dd4e-00f3-11ee-9f77-e8ebd339fe3c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:15:53,227 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:15:53,229 INFO     [orchestrator_api.py:376] new model id is aa7df2a6-00f3-11ee-a89c-e8ebd335ce06
2023-06-01 20:15:53,342 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 500 values (cache size 38700) in TrainAndInferHF
2023-06-01 20:15:53,343 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/aa7df2a6-00f3-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_949']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/aa7df2a6-00f3-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:15:54.557531: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:15:54.737560: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:15:55,581 INFO     [train_and_infer_hf.py:184] Infer hf model done
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:15:57,198 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:15:57,276 INFO     [train_and_infer_api.py:122] finished running infer for 3392 values
2023-06-01 20:15:57,666 INFO     [train_and_infer_api.py:122] finished running infer for 14541 values
2023-06-01 20:15:57,832 INFO     [disk_cache.py:28] saving 4585 items to disk cache took 0.5451571941375732
2023-06-01 20:15:58,012 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 68740) in TrainAndInferHF
2023-06-01 20:15:58,040 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:58,042 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:15:58,043 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:58,046 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:15:58,047 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:58,050 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:58,051 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:15:58,051 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:58,052 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:15:58,052 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:15:58,126 INFO     [train_and_dev_sets_selectors.py:27] using 800 for train using dataset polarity_imbalanced_positive_train and 588 for dev using dataset polarity_imbalanced_positive_dev
2023-06-01 20:15:58,127 INFO     [orchestrator_api.py:342] training a new model with {'false': 686, 'true': 114}
2023-06-01 20:15:58,128 INFO     [orchestrator_api.py:358] workspace imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM training a model for category 'positive', model_metadata: {'train_counts': {'false': 686, 'true': 114}, 'dev_counts': {'false': 529, 'true': 59}}
2023-06-01 20:15:58,134 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:15:59,492 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:15:59,513 INFO     [train_and_infer_api.py:122] finished running infer for 1043 values
2023-06-01 20:15:59,652 INFO     [disk_cache.py:28] saving 1039 items to disk cache took 0.13428139686584473
2023-06-01 20:15:59,708 INFO     [experiment_runner.py:179] Evaluation on dataset: cola_test, with AL: HARD_MINING, iteration: 10, repeat: 1, model (id: a214b9ec-00f3-11ee-8161-e8ebd33a184c) is: {'dataset': 'cola_test', 'category': '1', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 10, 'repeat id': 1, 'train positive count': 307, 'train negative count': 293, 'train total count': 600, 'BrierScore': 0.4037853666822923, 'LogLoss': 0.7555160580053703, 'confECE': 0.18340565367946837, 'cwECE': 0.17900992902254728, 'Acc': 76.79769894534996, 'MSE': nan, 'average_score': 0.9454070070788821, 'accuracy': 0.7679769894534996, 'precision': 0.7845238095238095, 'recall': 0.9152777777777777, 'f1': 0.8448717948717949, 'support': 720, 'tp': 659, 'fp': 181, 'tn': 142, 'fn': 61, 'diversity': 0.06429804242588741, 'representativeness': 0.3391582046933939}	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
2023-06-01 20:15:59,711 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 11, repeat num: 1	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
2023-06-01 20:15:59,845 INFO     [disk_cache.py:28] saving 17523 items to disk cache took 2.131685733795166
2023-06-01 20:15:59,865 INFO     [active_learning_api.py:59] Got 6977 unlabeled elements for active learning
2023-06-01 20:15:59.997112: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:15:59.997644: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:16:00,037 INFO     [train_and_infer_api.py:111] 15 already in cache, running infer for 6962 values (cache size 198643) in TrainAndInferHF
2023-06-01 20:16:00,044 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:16:00,280 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:16:00,293 INFO     [train_and_infer_api.py:122] finished running infer for 500 values
2023-06-01 20:16:00,301 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 141584) in TrainAndInferHF
2023-06-01 20:16:00,320 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:00,331 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:16:00,332 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:00,369 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:16:00,369 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:00,381 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:00,381 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:16:00,382 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:00,382 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:16:00,382 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:00,397 INFO     [disk_cache.py:28] saving 500 items to disk cache took 0.09975147247314453
2023-06-01 20:16:00,445 INFO     [experiment_runner.py:179] Evaluation on dataset: trec_test, with AL: RANDOM, iteration: 8, repeat: 1, model (id: aa7df2a6-00f3-11ee-a89c-e8ebd335ce06) is: {'dataset': 'trec_test', 'category': 'LOC', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 8, 'repeat id': 1, 'train positive count': 107, 'train negative count': 393, 'train total count': 500, 'BrierScore': 0.04091094245493166, 'LogLoss': 0.08356414629901061, 'confECE': 0.012453065908299914, 'cwECE': 0.012416117990855137, 'Acc': 97.6, 'MSE': nan, 'average_score': 0.9865706236362457, 'accuracy': 0.976, 'precision': 0.948051948051948, 'recall': 0.9012345679012346, 'f1': 0.9240506329113924, 'support': 81, 'tp': 73, 'fp': 4, 'tn': 415, 'fn': 8, 'diversity': 0.3269378432785562, 'representativeness': 0.5581562157860438}	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:16:00,447 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 9, repeat num: 1	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:16:00,560 INFO     [active_learning_api.py:59] Got 4118 unlabeled elements for active learning
2023-06-01 20:16:00,561 INFO     [train_and_dev_sets_selectors.py:27] using 500 for train using dataset ag_news_train and 2999 for dev using dataset ag_news_dev
2023-06-01 20:16:00,562 INFO     [orchestrator_api.py:342] training a new model with {'false': 385, 'true': 115}
2023-06-01 20:16:00,562 INFO     [orchestrator_api.py:358] workspace balanced_ag_news_calib_nseed_100_step_50_nactive_15_MC_10-ag_news-1-HFBERT-1-RANDOM training a model for category '1', model_metadata: {'train_counts': {'false': 385, 'true': 115}, 'dev_counts': {'false': 2209, 'true': 790}}
2023-06-01 20:16:00,582 INFO     [train_and_infer_hf.py:96] Training hf model...
2023-06-01 20:16:00,645 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: trec_train and category: LOC.	runtime: 0.19773125648498535	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/a214b9ec-00f3-11ee-8161-e8ebd33a184c were not used when initializing TFBertForSequenceClassification: ['dropout_2849']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/a214b9ec-00f3-11ee-8161-e8ebd33a184c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:16:01,969 INFO     [train_and_infer_api.py:111] 8 already in cache, running infer for 4166 values (cache size 39200) in TrainAndInferHF
2023-06-01 20:16:01,973 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:16:02.391358: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/aa7df2a6-00f3-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_949']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/aa7df2a6-00f3-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:16:03.620036: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:16:03.620532: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:16:04.002531: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:16:09,379 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:16:09,379 INFO     [orchestrator_api.py:376] new model id is af80f9ce-00f3-11ee-b1d5-e8ebd305f284
2023-06-01 20:16:12,093 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 2000 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:16:12,095 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/af80f9ce-00f3-11ee-b1d5-e8ebd305f284 were not used when initializing TFBertForSequenceClassification: ['dropout_3077']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/af80f9ce-00f3-11ee-b1d5-e8ebd305f284.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:16:13.851447: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:16:15.415641: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:16:17,751 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:16:17,792 INFO     [train_and_infer_api.py:122] finished running infer for 2513 values
2023-06-01 20:16:18,088 INFO     [disk_cache.py:28] saving 2513 items to disk cache took 0.28946828842163086
2023-06-01 20:16:18,171 INFO     [experiment_runner.py:179] Evaluation on dataset: ag_news_imbalanced_1_test, with AL: RANDOM, iteration: 7, repeat: 1, model (id: 9fa39e80-00f3-11ee-8ba6-1070fd5ed7e8) is: {'dataset': 'ag_news_imbalanced_1_test', 'category': '1', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 7, 'repeat id': 1, 'train positive count': 94, 'train negative count': 356, 'train total count': 450, 'BrierScore': 0.06367223801928694, 'LogLoss': 0.1286816987787559, 'confECE': 0.01785335473959724, 'cwECE': 0.020843269383583704, 'Acc': 96.25945085555114, 'MSE': nan, 'average_score': 0.9804478621890808, 'accuracy': 0.9625945085555113, 'precision': 0.819672131147541, 'recall': 0.8, 'f1': 0.8097165991902835, 'support': 250, 'tp': 200, 'fp': 44, 'tn': 2219, 'fn': 50, 'diversity': 0.3033926185283927, 'representativeness': 0.6841201451970561}	workspace: imbalanced_ag_news_imbalanced_1_calib_nseed_100_step_50_nactive_15_MC_10-ag_news_imbalanced_1-1-HFBERT-1-RANDOM
2023-06-01 20:16:18,174 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 8, repeat num: 1	workspace: imbalanced_ag_news_imbalanced_1_calib_nseed_100_step_50_nactive_15_MC_10-ag_news_imbalanced_1-1-HFBERT-1-RANDOM
2023-06-01 20:16:18,343 INFO     [active_learning_api.py:59] Got 12103 unlabeled elements for active learning
2023-06-01 20:16:18,436 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: ag_news_imbalanced_1_train and category: 1.	runtime: 0.2617452144622803	workspace: imbalanced_ag_news_imbalanced_1_calib_nseed_100_step_50_nactive_15_MC_10-ag_news_imbalanced_1-1-HFBERT-1-RANDOM
2023-06-01 20:16:20,509 INFO     [train_and_infer_api.py:111] 6 already in cache, running infer for 12113 values (cache size 106183) in TrainAndInferHF
2023-06-01 20:16:20,519 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/9fa39e80-00f3-11ee-8ba6-1070fd5ed7e8 were not used when initializing TFBertForSequenceClassification: ['dropout_835']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/9fa39e80-00f3-11ee-8ba6-1070fd5ed7e8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:16:27.756066: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:16:28.410751: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:16:30.969676: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:16:32,742 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:16:32,743 INFO     [orchestrator_api.py:376] new model id is c3ae3cea-00f3-11ee-a5c3-e8ebd329a878
2023-06-01 20:16:32,801 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1077 values (cache size 83655) in TrainAndInferHF
2023-06-01 20:16:32,802 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/c3ae3cea-00f3-11ee-a5c3-e8ebd329a878 were not used when initializing TFBertForSequenceClassification: ['dropout_2165']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/c3ae3cea-00f3-11ee-a5c3-e8ebd329a878.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:16:34.413490: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:16:35,931 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:16:35,963 INFO     [train_and_infer_api.py:122] finished running infer for 2000 values
2023-06-01 20:16:36,250 INFO     [disk_cache.py:28] saving 2000 items to disk cache took 0.2582998275756836
2023-06-01 20:16:36,318 INFO     [experiment_runner.py:179] Evaluation on dataset: subjectivity_test, with AL: HARD_MINING, iteration: 12, repeat: 1, model (id: af80f9ce-00f3-11ee-b1d5-e8ebd305f284) is: {'dataset': 'subjectivity_test', 'category': 'objective', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 12, 'repeat id': 1, 'train positive count': 349, 'train negative count': 351, 'train total count': 700, 'BrierScore': 0.07903601908900633, 'LogLoss': 0.15999567868088385, 'confECE': 0.02852161594380432, 'cwECE': 0.017057057091500617, 'Acc': 95.5, 'MSE': nan, 'average_score': 0.9822705605924129, 'accuracy': 0.955, 'precision': 0.9680511182108626, 'recall': 0.9380804953560371, 'f1': 0.9528301886792453, 'support': 969, 'tp': 909, 'fp': 30, 'tn': 1001, 'fn': 60, 'diversity': 0.054853180804018, 'representativeness': 0.16561983607243738}	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:16:36,323 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 13, repeat num: 1	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:16:36,485 INFO     [active_learning_api.py:59] Got 6300 unlabeled elements for active learning
2023-06-01 20:16:36,745 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 6300 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:16:36,757 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/af80f9ce-00f3-11ee-b1d5-e8ebd305f284 were not used when initializing TFBertForSequenceClassification: ['dropout_3077']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/af80f9ce-00f3-11ee-b1d5-e8ebd305f284.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:16:40.232166: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:16:46,401 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:16:46,418 INFO     [train_and_infer_api.py:122] finished running infer for 1077 values
2023-06-01 20:16:46,558 INFO     [disk_cache.py:28] saving 1077 items to disk cache took 0.1349644660949707
2023-06-01 20:16:46,597 INFO     [experiment_runner.py:179] Evaluation on dataset: subjectivity_imbalanced_subjective_test, with AL: HARD_MINING, iteration: 4, repeat: 1, model (id: c3ae3cea-00f3-11ee-a5c3-e8ebd329a878) is: {'dataset': 'subjectivity_imbalanced_subjective_test', 'category': 'subjective', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 4, 'repeat id': 1, 'train positive count': 89, 'train negative count': 211, 'train total count': 300, 'BrierScore': 0.11852196655359992, 'LogLoss': 0.1889399716746908, 'confECE': 0.044404064908331445, 'cwECE': 0.06468095226574798, 'Acc': 91.27205199628598, 'MSE': nan, 'average_score': 0.9360161807765093, 'accuracy': 0.9127205199628597, 'precision': 0.5380434782608695, 'recall': 0.9166666666666666, 'f1': 0.6780821917808219, 'support': 108, 'tp': 99, 'fp': 85, 'tn': 884, 'fn': 9, 'diversity': 0.05010223177091126, 'representativeness': 0.25255553292807886}	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:16:46,602 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 5, repeat num: 1	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:16:46,676 INFO     [active_learning_api.py:59] Got 3619 unlabeled elements for active learning
2023-06-01 20:16:46,814 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3619 values (cache size 84732) in TrainAndInferHF
2023-06-01 20:16:46,817 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/c3ae3cea-00f3-11ee-a5c3-e8ebd329a878 were not used when initializing TFBertForSequenceClassification: ['dropout_2165']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/c3ae3cea-00f3-11ee-a5c3-e8ebd329a878.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:16:49.350407: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:16:49,493 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:16:49,562 INFO     [train_and_infer_api.py:122] finished running infer for 4166 values
2023-06-01 20:16:50,111 INFO     [disk_cache.py:28] saving 4615 items to disk cache took 0.5267295837402344
2023-06-01 20:16:50,276 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 43315) in TrainAndInferHF
2023-06-01 20:16:50,295 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:50,298 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:16:50,298 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:50,303 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:16:50,304 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:50,309 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:50,310 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:16:50,311 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:50,312 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:16:50,312 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:16:50,383 INFO     [train_and_dev_sets_selectors.py:27] using 550 for train using dataset trec_train and 776 for dev using dataset trec_dev
2023-06-01 20:16:50,384 INFO     [orchestrator_api.py:342] training a new model with {'false': 436, 'true': 114}
2023-06-01 20:16:50,385 INFO     [orchestrator_api.py:358] workspace imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM training a model for category 'LOC', model_metadata: {'train_counts': {'false': 436, 'true': 114}, 'dev_counts': {'false': 658, 'true': 118}}
2023-06-01 20:16:50,391 INFO     [train_and_infer_hf.py:96] Training hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:16:51,438 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:16:51,438 INFO     [orchestrator_api.py:376] new model id is cb34b2be-00f3-11ee-b509-e8ebd329a958
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:16:51,596 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1193 values (cache size 68740) in TrainAndInferHF
2023-06-01 20:16:51,597 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:16:51.852284: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:16:51.852826: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/cb34b2be-00f3-11ee-b509-e8ebd329a958 were not used when initializing TFBertForSequenceClassification: ['dropout_1633']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/cb34b2be-00f3-11ee-b509-e8ebd329a958.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:16:53.453525: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:17:06,768 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:17:06,793 INFO     [train_and_infer_api.py:122] finished running infer for 1193 values
2023-06-01 20:17:06,956 INFO     [disk_cache.py:28] saving 1193 items to disk cache took 0.15660858154296875
2023-06-01 20:17:07,011 INFO     [experiment_runner.py:179] Evaluation on dataset: polarity_imbalanced_positive_test, with AL: RANDOM, iteration: 14, repeat: 1, model (id: cb34b2be-00f3-11ee-b509-e8ebd329a958) is: {'dataset': 'polarity_imbalanced_positive_test', 'category': 'positive', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 14, 'repeat id': 1, 'train positive count': 114, 'train negative count': 686, 'train total count': 800, 'BrierScore': 0.12998237148782057, 'LogLoss': 0.24012453835137273, 'confECE': 0.045138490330919795, 'cwECE': 0.047174713074144566, 'Acc': 91.95305951383068, 'MSE': nan, 'average_score': 0.9585506996984554, 'accuracy': 0.9195305951383068, 'precision': 0.5934959349593496, 'recall': 0.6134453781512605, 'f1': 0.6033057851239669, 'support': 119, 'tp': 73, 'fp': 50, 'tn': 1024, 'fn': 46, 'diversity': 0.19876018614511404, 'representativeness': 0.3214483563294177}	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM
2023-06-01 20:17:07,016 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 15, repeat num: 1	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM
2023-06-01 20:17:07,176 INFO     [active_learning_api.py:59] Got 3342 unlabeled elements for active learning
2023-06-01 20:17:07,315 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: polarity_imbalanced_positive_train and category: positive.	runtime: 0.2982065677642822	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM
2023-06-01 20:17:07,561 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3342 values (cache size 69933) in TrainAndInferHF
2023-06-01 20:17:07,564 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/cb34b2be-00f3-11ee-b509-e8ebd329a958 were not used when initializing TFBertForSequenceClassification: ['dropout_1633']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/cb34b2be-00f3-11ee-b509-e8ebd329a958.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:17:10.121309: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:17:10,285 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:17:10,286 INFO     [orchestrator_api.py:376] new model id is ccaa48f2-00f3-11ee-b311-e8ebd329a920
2023-06-01 20:17:10,461 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3000 values (cache size 141584) in TrainAndInferHF
2023-06-01 20:17:10,463 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/ccaa48f2-00f3-11ee-b311-e8ebd329a920 were not used when initializing TFBertForSequenceClassification: ['dropout_949']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/ccaa48f2-00f3-11ee-b311-e8ebd329a920.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:17:13,023 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:17:13.077113: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:13,134 INFO     [train_and_infer_api.py:122] finished running infer for 6913 values
2023-06-01 20:17:14,234 INFO     [disk_cache.py:28] saving 9046 items to disk cache took 1.040994644165039
2023-06-01 20:17:14,383 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: polarity_train and category: positive.	runtime: 83.6022469997406	workspace: balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING
2023-06-01 20:17:17,143 INFO     [train_and_infer_api.py:111] 6913 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:17:17,383 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:17:17,406 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:17,409 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:17,410 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:17,416 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:17,416 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:17,423 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:17,423 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:17,424 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:17,424 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:17,426 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:17,530 INFO     [train_and_dev_sets_selectors.py:27] using 600 for train using dataset polarity_train and 1066 for dev using dataset polarity_dev
2023-06-01 20:17:17,531 INFO     [orchestrator_api.py:342] training a new model with {'false': 337, 'true': 263}
2023-06-01 20:17:17,532 INFO     [orchestrator_api.py:358] workspace balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING training a model for category 'positive', model_metadata: {'train_counts': {'false': 337, 'true': 263}, 'dev_counts': {'true': 537, 'false': 529}}
2023-06-01 20:17:17,541 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:17:19.317283: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:19.318540: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:19.541431: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:17:19,896 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:17:20,032 INFO     [train_and_infer_api.py:122] finished running infer for 6962 values
2023-06-01 20:17:21,009 INFO     [disk_cache.py:28] saving 8001 items to disk cache took 0.909736156463623
2023-06-01 20:17:21,157 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: cola_train and category: 1.	runtime: 81.4446153640747	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
2023-06-01 20:17:21,307 INFO     [train_and_infer_api.py:111] 6992 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:17:21,549 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:17:21,572 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:21,574 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:21,575 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:21,582 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:21,582 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:21,592 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:21,593 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:21,593 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:21,594 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:21,595 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:21,698 INFO     [train_and_dev_sets_selectors.py:27] using 650 for train using dataset cola_train and 959 for dev using dataset cola_dev
2023-06-01 20:17:21,699 INFO     [orchestrator_api.py:342] training a new model with {'false': 317, 'true': 333}
2023-06-01 20:17:21,700 INFO     [orchestrator_api.py:358] workspace balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING training a model for category '1', model_metadata: {'train_counts': {'false': 317, 'true': 333}, 'dev_counts': {'true': 673, 'false': 286}}
2023-06-01 20:17:21,709 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:17:23.472569: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:23.473104: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:17:29,556 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:17:29,630 INFO     [train_and_infer_api.py:122] finished running infer for 3619 values
2023-06-01 20:17:30,176 INFO     [disk_cache.py:28] saving 4696 items to disk cache took 0.5307455062866211
2023-06-01 20:17:30,247 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: subjectivity_imbalanced_subjective_train and category: subjective.	runtime: 43.64505434036255	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:17:30,321 INFO     [train_and_infer_api.py:111] 3619 already in cache, running infer for 0 values (cache size 88351) in TrainAndInferHF
2023-06-01 20:17:30,445 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 88351) in TrainAndInferHF
2023-06-01 20:17:30,458 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:30,459 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:30,460 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:30,463 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:30,464 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:30,467 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:30,468 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:30,468 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:30,469 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:30,469 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:30,538 INFO     [train_and_dev_sets_selectors.py:27] using 350 for train using dataset subjectivity_imbalanced_subjective_train and 560 for dev using dataset subjectivity_imbalanced_subjective_dev
2023-06-01 20:17:30,539 INFO     [orchestrator_api.py:342] training a new model with {'false': 258, 'true': 92}
2023-06-01 20:17:30,540 INFO     [orchestrator_api.py:358] workspace imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING training a model for category 'subjective', model_metadata: {'train_counts': {'false': 258, 'true': 92}, 'dev_counts': {'false': 504, 'true': 56}}
2023-06-01 20:17:30,546 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:17:32.139241: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:32.139686: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:17:37,491 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:17:37,775 INFO     [train_and_infer_api.py:122] finished running infer for 14691 values
2023-06-01 20:17:39,888 INFO     [disk_cache.py:28] saving 17685 items to disk cache took 2.0616064071655273
2023-06-01 20:17:40,330 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 88925) in TrainAndInferHF
2023-06-01 20:17:40,345 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:40,350 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:40,351 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:40,364 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:40,365 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:40,381 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:40,382 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:40,382 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:40,383 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:40,383 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:40,562 INFO     [train_and_dev_sets_selectors.py:27] using 350 for train using dataset wiki_attack_train and 2998 for dev using dataset wiki_attack_dev
2023-06-01 20:17:40,562 INFO     [orchestrator_api.py:342] training a new model with {'false': 271, 'true': 79}
2023-06-01 20:17:40,563 INFO     [orchestrator_api.py:358] workspace imbalanced_wiki_attack_calib_nseed_100_step_50_nactive_15_MC_10-wiki_attack-True-HFBERT-1-RANDOM training a model for category 'True', model_metadata: {'train_counts': {'false': 271, 'true': 79}, 'dev_counts': {'false': 2676, 'true': 322}}
2023-06-01 20:17:40,590 INFO     [train_and_infer_hf.py:96] Training hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:17:40,771 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:17:40,772 INFO     [orchestrator_api.py:376] new model id is ea5a8088-00f3-11ee-a89c-e8ebd335ce06
2023-06-01 20:17:40,884 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 500 values (cache size 43315) in TrainAndInferHF
2023-06-01 20:17:40,885 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/ea5a8088-00f3-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_1063']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/ea5a8088-00f3-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:17:41.925023: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:45.440333: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:45.440873: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:17:46,516 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:17:46.567746: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:46,575 INFO     [train_and_infer_api.py:122] finished running infer for 3000 values
2023-06-01 20:17:46,937 INFO     [disk_cache.py:28] saving 3000 items to disk cache took 0.35086894035339355
2023-06-01 20:17:47,040 INFO     [experiment_runner.py:179] Evaluation on dataset: ag_news_test, with AL: RANDOM, iteration: 8, repeat: 1, model (id: ccaa48f2-00f3-11ee-b311-e8ebd329a920) is: {'dataset': 'ag_news_test', 'category': '1', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 8, 'repeat id': 1, 'train positive count': 115, 'train negative count': 385, 'train total count': 500, 'BrierScore': 0.1098327886381076, 'LogLoss': 0.2336163547386531, 'confECE': 0.04278079789050149, 'cwECE': 0.03755734453350307, 'Acc': 93.7, 'MSE': nan, 'average_score': 0.9779595872958501, 'accuracy': 0.937, 'precision': 0.8814504881450488, 'recall': 0.8586956521739131, 'f1': 0.869924294562973, 'support': 736, 'tp': 632, 'fp': 85, 'tn': 2179, 'fn': 104, 'diversity': 0.2524571466972371, 'representativeness': 0.437836805960106}	workspace: balanced_ag_news_calib_nseed_100_step_50_nactive_15_MC_10-ag_news-1-HFBERT-1-RANDOM
2023-06-01 20:17:47,042 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 9, repeat num: 1	workspace: balanced_ag_news_calib_nseed_100_step_50_nactive_15_MC_10-ag_news-1-HFBERT-1-RANDOM
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:17:47,047 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:17:47,103 INFO     [train_and_infer_api.py:122] finished running infer for 3342 values
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:17:47,568 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:17:47,588 INFO     [train_and_infer_api.py:122] finished running infer for 500 values
2023-06-01 20:17:47,682 INFO     [disk_cache.py:28] saving 4535 items to disk cache took 0.557260274887085
2023-06-01 20:17:47,684 INFO     [disk_cache.py:28] saving 500 items to disk cache took 0.0836327075958252
2023-06-01 20:17:47,733 INFO     [experiment_runner.py:179] Evaluation on dataset: trec_test, with AL: RANDOM, iteration: 9, repeat: 1, model (id: ea5a8088-00f3-11ee-a89c-e8ebd335ce06) is: {'dataset': 'trec_test', 'category': 'LOC', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 9, 'repeat id': 1, 'train positive count': 114, 'train negative count': 436, 'train total count': 550, 'BrierScore': 0.0787470231541609, 'LogLoss': 0.13101676412079555, 'confECE': 0.02209969600874499, 'cwECE': 0.04417499953368678, 'Acc': 94.6, 'MSE': nan, 'average_score': 0.968099695444107, 'accuracy': 0.946, 'precision': 0.7547169811320755, 'recall': 0.9876543209876543, 'f1': 0.8556149732620321, 'support': 81, 'tp': 80, 'fp': 26, 'tn': 393, 'fn': 1, 'diversity': 0.27262748908538575, 'representativeness': 0.47611023980622125}	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:17:47,736 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 10, repeat num: 1	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:17:47,860 INFO     [active_learning_api.py:59] Got 4068 unlabeled elements for active learning
2023-06-01 20:17:47,866 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 73275) in TrainAndInferHF
2023-06-01 20:17:47,895 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:47,897 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:47,898 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:47,901 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:47,902 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:47,905 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:47,906 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:47,906 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:47,907 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:47,908 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:47,952 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: trec_train and category: LOC.	runtime: 0.215895414352417	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:17:47,984 INFO     [train_and_dev_sets_selectors.py:27] using 850 for train using dataset polarity_imbalanced_positive_train and 588 for dev using dataset polarity_imbalanced_positive_dev
2023-06-01 20:17:47,984 INFO     [orchestrator_api.py:342] training a new model with {'false': 732, 'true': 118}
2023-06-01 20:17:47,985 INFO     [orchestrator_api.py:358] workspace imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM training a model for category 'positive', model_metadata: {'train_counts': {'false': 732, 'true': 118}, 'dev_counts': {'false': 529, 'true': 59}}
2023-06-01 20:17:47,991 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:17:49,484 INFO     [active_learning_api.py:59] Got 14482 unlabeled elements for active learning
2023-06-01 20:17:49,584 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: ag_news_train and category: 1.	runtime: 2.540731430053711	workspace: balanced_ag_news_calib_nseed_100_step_50_nactive_15_MC_10-ag_news-1-HFBERT-1-RANDOM
2023-06-01 20:17:50.006245: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:50.006789: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:50,268 INFO     [train_and_infer_api.py:111] 9 already in cache, running infer for 14491 values (cache size 144584) in TrainAndInferHF
2023-06-01 20:17:50,283 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:17:50,316 INFO     [train_and_infer_api.py:111] 8 already in cache, running infer for 4116 values (cache size 43815) in TrainAndInferHF
2023-06-01 20:17:50,319 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:17:50,517 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:17:50,610 INFO     [train_and_infer_api.py:122] finished running infer for 6300 values
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/ccaa48f2-00f3-11ee-b311-e8ebd329a920 were not used when initializing TFBertForSequenceClassification: ['dropout_949']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/ccaa48f2-00f3-11ee-b311-e8ebd329a920.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:17:51.415968: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/ea5a8088-00f3-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_1063']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/ea5a8088-00f3-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:17:51,578 INFO     [disk_cache.py:28] saving 8300 items to disk cache took 0.9147627353668213
2023-06-01 20:17:51,755 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: subjectivity_train and category: objective.	runtime: 75.43071699142456	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:17:51,912 INFO     [train_and_infer_api.py:111] 6300 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:17:52,152 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:17:52,180 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:52,182 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:52,182 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:52,188 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:52,189 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:52,197 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:52,197 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:17:52,197 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:52,198 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:17:52,198 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:17:52.293842: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:52,305 INFO     [train_and_dev_sets_selectors.py:27] using 750 for train using dataset subjectivity_train and 1000 for dev using dataset subjectivity_dev
2023-06-01 20:17:52,305 INFO     [orchestrator_api.py:342] training a new model with {'false': 374, 'true': 376}
2023-06-01 20:17:52,306 INFO     [orchestrator_api.py:358] workspace balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING training a model for category 'objective', model_metadata: {'train_counts': {'false': 374, 'true': 376}, 'dev_counts': {'true': 504, 'false': 496}}
2023-06-01 20:17:52,315 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:17:54.013740: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:54.014303: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:17:59.889046: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:18:00.738251: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:18:12,673 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:18:12,674 INFO     [orchestrator_api.py:376] new model id is fa89529a-00f3-11ee-9f77-e8ebd339fe3c
2023-06-01 20:18:12,830 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 2133 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:18:12,832 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/fa89529a-00f3-11ee-9f77-e8ebd339fe3c were not used when initializing TFBertForSequenceClassification: ['dropout_2849']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/fa89529a-00f3-11ee-9f77-e8ebd339fe3c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:18:13.979831: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:18:14.578417: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:18:19.010332: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:18:19,136 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:18:19,137 INFO     [orchestrator_api.py:376] new model id is fd056c0c-00f3-11ee-8161-e8ebd33a184c
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:18:20,644 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:18:20,645 INFO     [orchestrator_api.py:376] new model id is 0249c08c-00f4-11ee-a5c3-e8ebd329a878
2023-06-01 20:18:20,715 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1077 values (cache size 88351) in TrainAndInferHF
2023-06-01 20:18:20,716 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/0249c08c-00f4-11ee-a5c3-e8ebd329a878 were not used when initializing TFBertForSequenceClassification: ['dropout_2279']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/0249c08c-00f4-11ee-a5c3-e8ebd329a878.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:18:22.196850: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:18:22,234 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1043 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:18:22,287 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:18:22.289214: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/fd056c0c-00f3-11ee-8161-e8ebd33a184c were not used when initializing TFBertForSequenceClassification: ['dropout_2963']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/fd056c0c-00f3-11ee-8161-e8ebd33a184c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:18:24.227145: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:18:34,498 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:18:34,520 INFO     [train_and_infer_api.py:122] finished running infer for 1077 values
2023-06-01 20:18:34,667 INFO     [disk_cache.py:28] saving 1077 items to disk cache took 0.1424274444580078
2023-06-01 20:18:34,708 INFO     [experiment_runner.py:179] Evaluation on dataset: subjectivity_imbalanced_subjective_test, with AL: HARD_MINING, iteration: 5, repeat: 1, model (id: 0249c08c-00f4-11ee-a5c3-e8ebd329a878) is: {'dataset': 'subjectivity_imbalanced_subjective_test', 'category': 'subjective', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 5, 'repeat id': 1, 'train positive count': 92, 'train negative count': 258, 'train total count': 350, 'BrierScore': 0.06968137262067779, 'LogLoss': 0.14200104924002943, 'confECE': 0.03682978959307834, 'cwECE': 0.03888970051400067, 'Acc': 95.54317548746518, 'MSE': nan, 'average_score': 0.9345795430844873, 'accuracy': 0.9554317548746518, 'precision': 0.8488372093023255, 'recall': 0.6759259259259259, 'f1': 0.7525773195876289, 'support': 108, 'tp': 73, 'fp': 13, 'tn': 956, 'fn': 35, 'diversity': 0.03791223101109387, 'representativeness': 0.3707323745765433}	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:18:34,727 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 6, repeat num: 1	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:18:34,811 INFO     [active_learning_api.py:59] Got 3569 unlabeled elements for active learning
2023-06-01 20:18:34,956 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3569 values (cache size 89428) in TrainAndInferHF
2023-06-01 20:18:34,959 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:18:35,932 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:18:35,957 INFO     [train_and_infer_api.py:122] finished running infer for 1043 values
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/0249c08c-00f4-11ee-a5c3-e8ebd329a878 were not used when initializing TFBertForSequenceClassification: ['dropout_2279']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/0249c08c-00f4-11ee-a5c3-e8ebd329a878.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:18:36,104 INFO     [disk_cache.py:28] saving 1039 items to disk cache took 0.13444948196411133
2023-06-01 20:18:36,168 INFO     [experiment_runner.py:179] Evaluation on dataset: cola_test, with AL: HARD_MINING, iteration: 11, repeat: 1, model (id: fd056c0c-00f3-11ee-8161-e8ebd33a184c) is: {'dataset': 'cola_test', 'category': '1', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 11, 'repeat id': 1, 'train positive count': 333, 'train negative count': 317, 'train total count': 650, 'BrierScore': 0.39027011214285184, 'LogLoss': 0.7254584581201177, 'confECE': 0.16912959777307648, 'cwECE': 0.1782258443095947, 'Acc': 77.46883988494727, 'MSE': nan, 'average_score': 0.9438179965672991, 'accuracy': 0.7746883988494727, 'precision': 0.7715565509518477, 'recall': 0.9569444444444445, 'f1': 0.8543087414755114, 'support': 720, 'tp': 689, 'fp': 204, 'tn': 119, 'fn': 31, 'diversity': 0.06591070087107671, 'representativeness': 0.15845295526322703}	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
2023-06-01 20:18:36,181 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 12, repeat num: 1	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
2023-06-01 20:18:36,340 INFO     [active_learning_api.py:59] Got 6927 unlabeled elements for active learning
2023-06-01 20:18:36,508 INFO     [train_and_infer_api.py:111] 15 already in cache, running infer for 6912 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:18:36,516 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:18:37.432968: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:18:38,108 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:18:38,173 INFO     [train_and_infer_api.py:122] finished running infer for 4116 values
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/fd056c0c-00f3-11ee-8161-e8ebd33a184c were not used when initializing TFBertForSequenceClassification: ['dropout_2963']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/fd056c0c-00f3-11ee-8161-e8ebd33a184c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:18:38,718 INFO     [disk_cache.py:28] saving 4566 items to disk cache took 0.511775016784668
2023-06-01 20:18:38,891 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 47881) in TrainAndInferHF
2023-06-01 20:18:38,912 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:38,913 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:18:38,914 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:38,918 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:18:38,919 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:38,925 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:38,925 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:18:38,926 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:38,927 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:18:38,928 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:39,001 INFO     [train_and_dev_sets_selectors.py:27] using 600 for train using dataset trec_train and 776 for dev using dataset trec_dev
2023-06-01 20:18:39,002 INFO     [orchestrator_api.py:342] training a new model with {'false': 477, 'true': 123}
2023-06-01 20:18:39,003 INFO     [orchestrator_api.py:358] workspace imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM training a model for category 'LOC', model_metadata: {'train_counts': {'false': 477, 'true': 123}, 'dev_counts': {'false': 658, 'true': 118}}
2023-06-01 20:18:39,009 INFO     [train_and_infer_hf.py:96] Training hf model...
2023-06-01 20:18:39.473835: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:18:40,003 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:18:40,047 INFO     [train_and_infer_api.py:122] finished running infer for 2133 values
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:18:40,327 INFO     [disk_cache.py:28] saving 2133 items to disk cache took 0.26184964179992676
2023-06-01 20:18:40,394 INFO     [experiment_runner.py:179] Evaluation on dataset: polarity_test, with AL: HARD_MINING, iteration: 10, repeat: 1, model (id: fa89529a-00f3-11ee-9f77-e8ebd339fe3c) is: {'dataset': 'polarity_test', 'category': 'positive', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 10, 'repeat id': 1, 'train positive count': 263, 'train negative count': 337, 'train total count': 600, 'BrierScore': 0.27872704423871913, 'LogLoss': 0.50198237372685, 'confECE': 0.10506987528468736, 'cwECE': 0.10931259752139869, 'Acc': 83.26300984528832, 'MSE': nan, 'average_score': 0.9327693966091154, 'accuracy': 0.8326300984528833, 'precision': 0.8865638766519823, 'recall': 0.7601510859301227, 'f1': 0.8185053380782918, 'support': 1059, 'tp': 805, 'fp': 103, 'tn': 971, 'fn': 254, 'diversity': 0.05836409211634469, 'representativeness': 0.30062795518450347}	workspace: balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING
2023-06-01 20:18:40,398 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 11, repeat num: 1	workspace: balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING
2023-06-01 20:18:40.461657: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:18:40.462115: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:18:40,551 INFO     [active_learning_api.py:59] Got 6863 unlabeled elements for active learning
2023-06-01 20:18:40,824 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 6863 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:18:40,831 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:18:40,832 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:18:41,045 INFO     [train_and_infer_api.py:122] finished running infer for 12113 values
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/fa89529a-00f3-11ee-9f77-e8ebd339fe3c were not used when initializing TFBertForSequenceClassification: ['dropout_2849']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/fa89529a-00f3-11ee-9f77-e8ebd339fe3c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:18:42,604 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:18:42,605 INFO     [orchestrator_api.py:376] new model id is 0caf8bd8-00f4-11ee-b509-e8ebd329a958
2023-06-01 20:18:42,717 INFO     [disk_cache.py:28] saving 14610 items to disk cache took 1.6388161182403564
2023-06-01 20:18:42,776 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1193 values (cache size 73275) in TrainAndInferHF
2023-06-01 20:18:42,778 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:18:43,096 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 118280) in TrainAndInferHF
2023-06-01 20:18:43,116 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:43,120 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:18:43,121 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:43,131 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:18:43,131 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:43,144 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:43,144 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:18:43,145 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:43,146 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:18:43,146 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:43,303 INFO     [train_and_dev_sets_selectors.py:27] using 500 for train using dataset ag_news_imbalanced_1_train and 2455 for dev using dataset ag_news_imbalanced_1_dev
2023-06-01 20:18:43,303 INFO     [orchestrator_api.py:342] training a new model with {'false': 401, 'true': 99}
2023-06-01 20:18:43,305 INFO     [orchestrator_api.py:358] workspace imbalanced_ag_news_imbalanced_1_calib_nseed_100_step_50_nactive_15_MC_10-ag_news_imbalanced_1-1-HFBERT-1-RANDOM training a model for category '1', model_metadata: {'train_counts': {'false': 401, 'true': 99}, 'dev_counts': {'false': 2209, 'true': 246}}
2023-06-01 20:18:43,319 INFO     [train_and_infer_hf.py:96] Training hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/0caf8bd8-00f4-11ee-b509-e8ebd329a958 were not used when initializing TFBertForSequenceClassification: ['dropout_1747']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/0caf8bd8-00f4-11ee-b509-e8ebd329a958.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:18:44.244769: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:18:44.297563: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:18:45.959516: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:18:45.960576: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:18:48,876 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:18:48,876 INFO     [orchestrator_api.py:376] new model id is 0f43643c-00f4-11ee-b1d5-e8ebd305f284
2023-06-01 20:18:51,746 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 2000 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:18:51,748 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/0f43643c-00f4-11ee-b1d5-e8ebd305f284 were not used when initializing TFBertForSequenceClassification: ['dropout_3191']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/0f43643c-00f4-11ee-b1d5-e8ebd305f284.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:18:53.440433: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:18:56,549 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:18:56,550 INFO     [orchestrator_api.py:376] new model id is 08464172-00f4-11ee-9023-e8ebd329a838
2023-06-01 20:18:56,687 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3000 values (cache size 88925) in TrainAndInferHF
2023-06-01 20:18:56,690 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:18:57,563 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:18:57,585 INFO     [train_and_infer_api.py:122] finished running infer for 1193 values
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/08464172-00f4-11ee-9023-e8ebd329a838 were not used when initializing TFBertForSequenceClassification: ['dropout_607']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/08464172-00f4-11ee-9023-e8ebd329a838.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:18:57,750 INFO     [disk_cache.py:28] saving 1193 items to disk cache took 0.15569758415222168
2023-06-01 20:18:57,811 INFO     [experiment_runner.py:179] Evaluation on dataset: polarity_imbalanced_positive_test, with AL: RANDOM, iteration: 15, repeat: 1, model (id: 0caf8bd8-00f4-11ee-b509-e8ebd329a958) is: {'dataset': 'polarity_imbalanced_positive_test', 'category': 'positive', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 15, 'repeat id': 1, 'train positive count': 118, 'train negative count': 732, 'train total count': 850, 'BrierScore': 0.17440813012771098, 'LogLoss': 0.3552694461955795, 'confECE': 0.07585028754174697, 'cwECE': 0.07582120681673363, 'Acc': 90.02514668901928, 'MSE': nan, 'average_score': 0.9752105165706996, 'accuracy': 0.9002514668901928, 'precision': 0.5, 'recall': 0.4789915966386555, 'f1': 0.4892703862660944, 'support': 119, 'tp': 57, 'fp': 57, 'tn': 1017, 'fn': 62, 'diversity': 0.20165245644168978, 'representativeness': 0.3382181835986754}	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM
2023-06-01 20:18:57,814 INFO     [orchestrator_api.py:145] deleting workspace imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-RANDOM ignore errors False
2023-06-01 20:18:58,367 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 1, repeat num: 1	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING
2023-06-01 20:18:58,408 INFO     [active_learning_api.py:59] Got 4042 unlabeled elements for active learning
2023-06-01 20:18:58,418 INFO     [train_and_infer_api.py:111] 4042 already in cache, running infer for 0 values (cache size 74468) in TrainAndInferHF
2023-06-01 20:18:58,443 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: polarity_imbalanced_positive_train and category: positive.	runtime: 0.07501387596130371	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING
2023-06-01 20:18:59,787 INFO     [train_and_infer_api.py:111] 4042 already in cache, running infer for 0 values (cache size 74468) in TrainAndInferHF
2023-06-01 20:18:59,905 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 74468) in TrainAndInferHF
2023-06-01 20:18:59,912 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:59,913 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:18:59,913 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:59,921 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:18:59,921 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:59,925 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:59,925 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:18:59,926 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:59,927 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:18:59,927 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:18:59,980 INFO     [train_and_dev_sets_selectors.py:27] using 150 for train using dataset polarity_imbalanced_positive_train and 588 for dev using dataset polarity_imbalanced_positive_dev
2023-06-01 20:18:59,980 INFO     [orchestrator_api.py:342] training a new model with {'false': 99, 'true': 51}
2023-06-01 20:18:59,981 INFO     [orchestrator_api.py:358] workspace imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING training a model for category 'positive', model_metadata: {'train_counts': {'false': 99, 'true': 51}, 'dev_counts': {'false': 529, 'true': 59}}
2023-06-01 20:18:59,987 INFO     [train_and_infer_hf.py:96] Training hf model...
2023-06-01 20:19:00.954371: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:19:01.904788: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:19:01.905473: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:19:08.458957: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:19:14.118334: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:19:16,485 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:19:16,522 INFO     [train_and_infer_api.py:122] finished running infer for 2000 values
2023-06-01 20:19:16,813 INFO     [disk_cache.py:28] saving 2000 items to disk cache took 0.2738621234893799
2023-06-01 20:19:16,884 INFO     [experiment_runner.py:179] Evaluation on dataset: subjectivity_test, with AL: HARD_MINING, iteration: 13, repeat: 1, model (id: 0f43643c-00f4-11ee-b1d5-e8ebd305f284) is: {'dataset': 'subjectivity_test', 'category': 'objective', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 13, 'repeat id': 1, 'train positive count': 376, 'train negative count': 374, 'train total count': 750, 'BrierScore': 0.08014194554713905, 'LogLoss': 0.16357237876260244, 'confECE': 0.028818176890548337, 'cwECE': 0.029584112033946442, 'Acc': 95.25, 'MSE': nan, 'average_score': 0.9813181750476361, 'accuracy': 0.9525, 'precision': 0.9326732673267327, 'recall': 0.9721362229102167, 'f1': 0.9519959575543203, 'support': 969, 'tp': 942, 'fp': 68, 'tn': 963, 'fn': 27, 'diversity': 0.06495733448607767, 'representativeness': 0.14017618069555082}	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:19:16,888 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 14, repeat num: 1	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:19:17,065 INFO     [active_learning_api.py:59] Got 6250 unlabeled elements for active learning
2023-06-01 20:19:17,328 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 6250 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:19:17,334 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:19:17,704 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:19:17,773 INFO     [train_and_infer_api.py:122] finished running infer for 3569 values
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/0f43643c-00f4-11ee-b1d5-e8ebd305f284 were not used when initializing TFBertForSequenceClassification: ['dropout_3191']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/0f43643c-00f4-11ee-b1d5-e8ebd305f284.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:19:18,299 INFO     [disk_cache.py:28] saving 4646 items to disk cache took 0.5110926628112793
2023-06-01 20:19:18,383 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: subjectivity_imbalanced_subjective_train and category: subjective.	runtime: 43.655492067337036	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:19:19,967 INFO     [train_and_infer_api.py:111] 3569 already in cache, running infer for 0 values (cache size 92997) in TrainAndInferHF
2023-06-01 20:19:20,095 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 92997) in TrainAndInferHF
2023-06-01 20:19:20,109 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:20,110 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:19:20,111 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:20,115 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:19:20,115 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:20,118 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:20,119 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:19:20,119 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:20,120 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:19:20,120 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:20,186 INFO     [train_and_dev_sets_selectors.py:27] using 400 for train using dataset subjectivity_imbalanced_subjective_train and 560 for dev using dataset subjectivity_imbalanced_subjective_dev
2023-06-01 20:19:20,187 INFO     [orchestrator_api.py:342] training a new model with {'false': 269, 'true': 131}
2023-06-01 20:19:20,188 INFO     [orchestrator_api.py:358] workspace imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING training a model for category 'subjective', model_metadata: {'train_counts': {'false': 269, 'true': 131}, 'dev_counts': {'false': 504, 'true': 56}}
2023-06-01 20:19:20,193 INFO     [train_and_infer_hf.py:96] Training hf model...
2023-06-01 20:19:20.754572: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:19:21.727259: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:19:21.727779: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:19:27.483642: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:19:31,829 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:19:31,830 INFO     [orchestrator_api.py:376] new model id is 2b1858b6-00f4-11ee-a89c-e8ebd335ce06
2023-06-01 20:19:32,000 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 500 values (cache size 47881) in TrainAndInferHF
2023-06-01 20:19:32,001 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/2b1858b6-00f4-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_1177']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/2b1858b6-00f4-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:19:33.108306: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:19:34,071 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:19:34,130 INFO     [train_and_infer_api.py:122] finished running infer for 3000 values
2023-06-01 20:19:34,493 INFO     [disk_cache.py:28] saving 2999 items to disk cache took 0.34886789321899414
2023-06-01 20:19:34,568 INFO     [experiment_runner.py:179] Evaluation on dataset: wiki_attack_test, with AL: RANDOM, iteration: 5, repeat: 1, model (id: 08464172-00f4-11ee-9023-e8ebd329a838) is: {'dataset': 'wiki_attack_test', 'category': 'True', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 5, 'repeat id': 1, 'train positive count': 79, 'train negative count': 271, 'train total count': 350, 'BrierScore': 0.12265268882570808, 'LogLoss': 0.22558641350391456, 'confECE': 0.018289860046893334, 'cwECE': 0.054238379172980786, 'Acc': 92.7, 'MSE': nan, 'average_score': 0.9248081771532695, 'accuracy': 0.927, 'precision': 0.8863636363636364, 'recall': 0.4394366197183099, 'f1': 0.5875706214689266, 'support': 355, 'tp': 156, 'fp': 20, 'tn': 2625, 'fn': 199, 'diversity': 0.23024641050238945, 'representativeness': 0.4331409257584481}	workspace: imbalanced_wiki_attack_calib_nseed_100_step_50_nactive_15_MC_10-wiki_attack-True-HFBERT-1-RANDOM
2023-06-01 20:19:34,571 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 6, repeat num: 1	workspace: imbalanced_wiki_attack_calib_nseed_100_step_50_nactive_15_MC_10-wiki_attack-True-HFBERT-1-RANDOM
2023-06-01 20:19:34,742 INFO     [active_learning_api.py:59] Got 14641 unlabeled elements for active learning
2023-06-01 20:19:34,817 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: wiki_attack_train and category: True.	runtime: 0.24516844749450684	workspace: imbalanced_wiki_attack_calib_nseed_100_step_50_nactive_15_MC_10-wiki_attack-True-HFBERT-1-RANDOM
2023-06-01 20:19:37,366 INFO     [train_and_infer_api.py:111] 9 already in cache, running infer for 14641 values (cache size 91924) in TrainAndInferHF
2023-06-01 20:19:37,378 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/08464172-00f4-11ee-9023-e8ebd329a838 were not used when initializing TFBertForSequenceClassification: ['dropout_607']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/08464172-00f4-11ee-9023-e8ebd329a838.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:19:38,708 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:19:38,718 INFO     [train_and_infer_api.py:122] finished running infer for 500 values
2023-06-01 20:19:38,790 INFO     [disk_cache.py:28] saving 500 items to disk cache took 0.0684819221496582
2023-06-01 20:19:38,841 INFO     [experiment_runner.py:179] Evaluation on dataset: trec_test, with AL: RANDOM, iteration: 10, repeat: 1, model (id: 2b1858b6-00f4-11ee-a89c-e8ebd335ce06) is: {'dataset': 'trec_test', 'category': 'LOC', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 10, 'repeat id': 1, 'train positive count': 123, 'train negative count': 477, 'train total count': 600, 'BrierScore': 0.0564874760726879, 'LogLoss': 0.11845086283947458, 'confECE': 0.02022855803026217, 'cwECE': 0.01263764053303748, 'Acc': 96.8, 'MSE': nan, 'average_score': 0.9869768813848495, 'accuracy': 0.968, 'precision': 0.9012345679012346, 'recall': 0.9012345679012346, 'f1': 0.9012345679012346, 'support': 81, 'tp': 73, 'fp': 8, 'tn': 411, 'fn': 8, 'diversity': 0.36252598553253657, 'representativeness': 0.5924577145278265}	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:19:38,843 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 11, repeat num: 1	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:19:38,969 INFO     [active_learning_api.py:59] Got 4018 unlabeled elements for active learning
2023-06-01 20:19:39,072 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: trec_train and category: LOC.	runtime: 0.2283327579498291	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:19:40,520 INFO     [train_and_infer_api.py:111] 8 already in cache, running infer for 4066 values (cache size 48381) in TrainAndInferHF
2023-06-01 20:19:40,524 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/2b1858b6-00f4-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_1177']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/2b1858b6-00f4-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:19:42.316416: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:19:45,853 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:19:45,853 INFO     [orchestrator_api.py:376] new model id is 3799429e-00f4-11ee-b509-e8ebd329a958
2023-06-01 20:19:45,882 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1193 values (cache size 74468) in TrainAndInferHF
2023-06-01 20:19:45,883 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:19:46,184 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:19:46,185 INFO     [orchestrator_api.py:376] new model id is 2daa1060-00f4-11ee-8ba6-1070fd5ed7e8
2023-06-01 20:19:46,349 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 2513 values (cache size 118280) in TrainAndInferHF
2023-06-01 20:19:46,351 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/3799429e-00f4-11ee-b509-e8ebd329a958 were not used when initializing TFBertForSequenceClassification: ['dropout_1823']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/3799429e-00f4-11ee-b509-e8ebd329a958.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:19:47.236256: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/2daa1060-00f4-11ee-8ba6-1070fd5ed7e8 were not used when initializing TFBertForSequenceClassification: ['dropout_949']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/2daa1060-00f4-11ee-8ba6-1070fd5ed7e8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:19:48.138972: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:19:48.766535: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:19:54.004464: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:19:56,957 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:19:57,079 INFO     [train_and_infer_api.py:122] finished running infer for 6912 values
2023-06-01 20:19:58,142 INFO     [disk_cache.py:28] saving 7951 items to disk cache took 0.9621577262878418
2023-06-01 20:19:58,291 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: cola_train and category: 1.	runtime: 82.10909008979797	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
2023-06-01 20:19:58,446 INFO     [train_and_infer_api.py:111] 6942 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:19:58,697 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:19:58,721 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:58,724 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:19:58,724 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:58,730 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:19:58,731 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:58,742 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:58,743 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:19:58,743 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:58,744 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:19:58,744 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:19:58,851 INFO     [train_and_dev_sets_selectors.py:27] using 700 for train using dataset cola_train and 959 for dev using dataset cola_dev
2023-06-01 20:19:58,852 INFO     [orchestrator_api.py:342] training a new model with {'false': 346, 'true': 354}
2023-06-01 20:19:58,853 INFO     [orchestrator_api.py:358] workspace balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING training a model for category '1', model_metadata: {'train_counts': {'false': 346, 'true': 354}, 'dev_counts': {'true': 673, 'false': 286}}
2023-06-01 20:19:58,862 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:20:00,519 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:20:00,541 INFO     [train_and_infer_api.py:122] finished running infer for 1193 values
2023-06-01 20:20:00.603204: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:00.604005: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:00,714 INFO     [disk_cache.py:28] saving 1193 items to disk cache took 0.1647961139678955
2023-06-01 20:20:00,747 INFO     [experiment_runner.py:179] Evaluation on dataset: polarity_imbalanced_positive_test, with AL: HARD_MINING, iteration: 1, repeat: 1, model (id: 3799429e-00f4-11ee-b509-e8ebd329a958) is: {'dataset': 'polarity_imbalanced_positive_test', 'category': 'positive', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 1, 'repeat id': 1, 'train positive count': 51, 'train negative count': 99, 'train total count': 150, 'BrierScore': 0.2160532014841576, 'LogLoss': 0.3617888974771267, 'confECE': 0.07253258462492908, 'cwECE': 0.1734352119498061, 'Acc': 85.24727577535624, 'MSE': nan, 'average_score': 0.7875838697905913, 'accuracy': 0.8524727577535625, 'precision': 0.35960591133004927, 'recall': 0.6134453781512605, 'f1': 0.453416149068323, 'support': 119, 'tp': 73, 'fp': 130, 'tn': 944, 'fn': 46, 'diversity': 0.09326843659347181, 'representativeness': 0.26678780032876714}	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING
2023-06-01 20:20:00,750 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 2, repeat num: 1	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING
2023-06-01 20:20:00,805 INFO     [active_learning_api.py:59] Got 3992 unlabeled elements for active learning
2023-06-01 20:20:00,960 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3992 values (cache size 75661) in TrainAndInferHF
2023-06-01 20:20:00,964 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/3799429e-00f4-11ee-b509-e8ebd329a958 were not used when initializing TFBertForSequenceClassification: ['dropout_1823']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/3799429e-00f4-11ee-b509-e8ebd329a958.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:20:03.483527: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:20:03,968 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:20:04,081 INFO     [train_and_infer_api.py:122] finished running infer for 6863 values
2023-06-01 20:20:05,219 INFO     [disk_cache.py:28] saving 8996 items to disk cache took 1.0825626850128174
2023-06-01 20:20:05,374 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: polarity_train and category: positive.	runtime: 84.97535800933838	workspace: balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING
2023-06-01 20:20:08,060 INFO     [train_and_infer_api.py:111] 6863 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:20:08,191 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:20:08,192 INFO     [orchestrator_api.py:376] new model id is 43a474e6-00f4-11ee-a5c3-e8ebd329a878
2023-06-01 20:20:08,274 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1077 values (cache size 92997) in TrainAndInferHF
2023-06-01 20:20:08,276 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:20:08,300 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:20:08,325 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:08,327 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:20:08,328 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:08,334 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:20:08,334 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:08,339 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:08,339 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:20:08,340 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:08,340 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:20:08,341 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:08,456 INFO     [train_and_dev_sets_selectors.py:27] using 650 for train using dataset polarity_train and 1066 for dev using dataset polarity_dev
2023-06-01 20:20:08,456 INFO     [orchestrator_api.py:342] training a new model with {'false': 357, 'true': 293}
2023-06-01 20:20:08,457 INFO     [orchestrator_api.py:358] workspace balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING training a model for category 'positive', model_metadata: {'train_counts': {'false': 357, 'true': 293}, 'dev_counts': {'true': 537, 'false': 529}}
2023-06-01 20:20:08,467 INFO     [train_and_infer_hf.py:96] Training hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/43a474e6-00f4-11ee-a5c3-e8ebd329a878 were not used when initializing TFBertForSequenceClassification: ['dropout_2393']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/43a474e6-00f4-11ee-a5c3-e8ebd329a878.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:20:10.193649: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:10.194360: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:11.222222: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:20:16,675 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:20:16,719 INFO     [train_and_infer_api.py:122] finished running infer for 2513 values
2023-06-01 20:20:17,028 INFO     [disk_cache.py:28] saving 2513 items to disk cache took 0.301239013671875
2023-06-01 20:20:17,110 INFO     [experiment_runner.py:179] Evaluation on dataset: ag_news_imbalanced_1_test, with AL: RANDOM, iteration: 8, repeat: 1, model (id: 2daa1060-00f4-11ee-8ba6-1070fd5ed7e8) is: {'dataset': 'ag_news_imbalanced_1_test', 'category': '1', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 8, 'repeat id': 1, 'train positive count': 99, 'train negative count': 401, 'train total count': 500, 'BrierScore': 0.09067620549059266, 'LogLoss': 0.16673391071155336, 'confECE': 0.02609596908442138, 'cwECE': 0.040682550906798495, 'Acc': 94.30959013131715, 'MSE': nan, 'average_score': 0.9691918698431152, 'accuracy': 0.9430959013131716, 'precision': 0.6646153846153846, 'recall': 0.864, 'f1': 0.7513043478260869, 'support': 250, 'tp': 216, 'fp': 109, 'tn': 2154, 'fn': 34, 'diversity': 0.23999568113778078, 'representativeness': 0.44655485713627435}	workspace: imbalanced_ag_news_imbalanced_1_calib_nseed_100_step_50_nactive_15_MC_10-ag_news_imbalanced_1-1-HFBERT-1-RANDOM
2023-06-01 20:20:17,112 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 9, repeat num: 1	workspace: imbalanced_ag_news_imbalanced_1_calib_nseed_100_step_50_nactive_15_MC_10-ag_news_imbalanced_1-1-HFBERT-1-RANDOM
2023-06-01 20:20:17,286 INFO     [active_learning_api.py:59] Got 12053 unlabeled elements for active learning
2023-06-01 20:20:17,388 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: ag_news_imbalanced_1_train and category: 1.	runtime: 0.27461767196655273	workspace: imbalanced_ag_news_imbalanced_1_calib_nseed_100_step_50_nactive_15_MC_10-ag_news_imbalanced_1-1-HFBERT-1-RANDOM
2023-06-01 20:20:19,562 INFO     [train_and_infer_api.py:111] 6 already in cache, running infer for 12063 values (cache size 120793) in TrainAndInferHF
2023-06-01 20:20:19,571 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/2daa1060-00f4-11ee-8ba6-1070fd5ed7e8 were not used when initializing TFBertForSequenceClassification: ['dropout_949']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/2daa1060-00f4-11ee-8ba6-1070fd5ed7e8.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:20:23,364 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:20:23,383 INFO     [train_and_infer_api.py:122] finished running infer for 1077 values
2023-06-01 20:20:23,516 INFO     [disk_cache.py:28] saving 1077 items to disk cache took 0.12806081771850586
2023-06-01 20:20:23,561 INFO     [experiment_runner.py:179] Evaluation on dataset: subjectivity_imbalanced_subjective_test, with AL: HARD_MINING, iteration: 6, repeat: 1, model (id: 43a474e6-00f4-11ee-a5c3-e8ebd329a878) is: {'dataset': 'subjectivity_imbalanced_subjective_test', 'category': 'subjective', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 6, 'repeat id': 1, 'train positive count': 131, 'train negative count': 269, 'train total count': 400, 'BrierScore': 0.06532515457035326, 'LogLoss': 0.13472416912864058, 'confECE': 0.02198789520979704, 'cwECE': 0.02009284406028751, 'Acc': 96.10027855153204, 'MSE': nan, 'average_score': 0.9829906811289136, 'accuracy': 0.9610027855153204, 'precision': 0.8837209302325582, 'recall': 0.7037037037037037, 'f1': 0.7835051546391752, 'support': 108, 'tp': 76, 'fp': 10, 'tn': 959, 'fn': 32, 'diversity': 0.06257891545915364, 'representativeness': 0.4744135533410057}	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:20:23,565 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 7, repeat num: 1	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:20:23,658 INFO     [active_learning_api.py:59] Got 3519 unlabeled elements for active learning
2023-06-01 20:20:23,805 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3519 values (cache size 94074) in TrainAndInferHF
2023-06-01 20:20:23,808 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/43a474e6-00f4-11ee-a5c3-e8ebd329a878 were not used when initializing TFBertForSequenceClassification: ['dropout_2393']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/43a474e6-00f4-11ee-a5c3-e8ebd329a878.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:20:26.134001: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:26.841695: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:20:27,285 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:20:27,369 INFO     [train_and_infer_api.py:122] finished running infer for 4066 values
2023-06-01 20:20:27,883 INFO     [disk_cache.py:28] saving 4517 items to disk cache took 0.5007514953613281
2023-06-01 20:20:28,064 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 52398) in TrainAndInferHF
2023-06-01 20:20:28,086 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:28,088 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:20:28,088 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:28,092 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:20:28,093 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:28,102 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:28,103 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:20:28,103 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:28,104 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:20:28,105 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:28,179 INFO     [train_and_dev_sets_selectors.py:27] using 650 for train using dataset trec_train and 776 for dev using dataset trec_dev
2023-06-01 20:20:28,179 INFO     [orchestrator_api.py:342] training a new model with {'false': 524, 'true': 126}
2023-06-01 20:20:28,180 INFO     [orchestrator_api.py:358] workspace imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM training a model for category 'LOC', model_metadata: {'train_counts': {'false': 524, 'true': 126}, 'dev_counts': {'false': 658, 'true': 118}}
2023-06-01 20:20:28,186 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:20:29.257766: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:29.560177: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:29.560671: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:20:32,928 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:20:33,026 INFO     [train_and_infer_api.py:122] finished running infer for 6250 values
2023-06-01 20:20:33,982 INFO     [disk_cache.py:28] saving 8250 items to disk cache took 0.9022769927978516
2023-06-01 20:20:34,164 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: subjectivity_train and category: objective.	runtime: 77.27467823028564	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:20:34,323 INFO     [train_and_infer_api.py:111] 6250 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:20:34,567 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:20:34,595 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:34,597 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:20:34,597 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:34,602 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:20:34,602 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:34,609 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:34,609 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:20:34,610 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:34,610 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:20:34,611 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:34,732 INFO     [train_and_dev_sets_selectors.py:27] using 800 for train using dataset subjectivity_train and 1000 for dev using dataset subjectivity_dev
2023-06-01 20:20:34,733 INFO     [orchestrator_api.py:342] training a new model with {'false': 414, 'true': 386}
2023-06-01 20:20:34,734 INFO     [orchestrator_api.py:358] workspace balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING training a model for category 'objective', model_metadata: {'train_counts': {'false': 414, 'true': 386}, 'dev_counts': {'true': 504, 'false': 496}}
2023-06-01 20:20:34,742 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:20:36.524875: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:36.525413: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:38.401163: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:20:41,764 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:20:42,060 INFO     [train_and_infer_api.py:122] finished running infer for 14491 values
2023-06-01 20:20:44,192 INFO     [disk_cache.py:28] saving 17473 items to disk cache took 2.091346502304077
2023-06-01 20:20:44,654 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 159057) in TrainAndInferHF
2023-06-01 20:20:44,676 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:44,680 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:20:44,680 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:44,692 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:20:44,692 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:44,710 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:44,710 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:20:44,711 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:44,711 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:20:44,712 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:44,893 INFO     [train_and_dev_sets_selectors.py:27] using 550 for train using dataset ag_news_train and 2999 for dev using dataset ag_news_dev
2023-06-01 20:20:44,893 INFO     [orchestrator_api.py:342] training a new model with {'false': 425, 'true': 125}
2023-06-01 20:20:44,894 INFO     [orchestrator_api.py:358] workspace balanced_ag_news_calib_nseed_100_step_50_nactive_15_MC_10-ag_news-1-HFBERT-1-RANDOM training a model for category '1', model_metadata: {'train_counts': {'false': 425, 'true': 125}, 'dev_counts': {'false': 2209, 'true': 790}}
2023-06-01 20:20:44,914 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:20:47,575 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:20:47,654 INFO     [train_and_infer_api.py:122] finished running infer for 3992 values
2023-06-01 20:20:47.928218: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:47.929010: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:48,256 INFO     [disk_cache.py:28] saving 5185 items to disk cache took 0.5790412425994873
2023-06-01 20:20:48,301 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: polarity_imbalanced_positive_train and category: positive.	runtime: 47.55067729949951	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING
2023-06-01 20:20:48,365 INFO     [train_and_infer_api.py:111] 3992 already in cache, running infer for 0 values (cache size 79653) in TrainAndInferHF
2023-06-01 20:20:48,480 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 79653) in TrainAndInferHF
2023-06-01 20:20:48,487 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:48,489 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:20:48,489 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:48,492 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:20:48,493 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:48,496 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:48,497 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:20:48,497 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:48,498 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:20:48,499 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:20:48,550 INFO     [train_and_dev_sets_selectors.py:27] using 200 for train using dataset polarity_imbalanced_positive_train and 588 for dev using dataset polarity_imbalanced_positive_dev
2023-06-01 20:20:48,550 INFO     [orchestrator_api.py:342] training a new model with {'false': 144, 'true': 56}
2023-06-01 20:20:48,551 INFO     [orchestrator_api.py:358] workspace imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING training a model for category 'positive', model_metadata: {'train_counts': {'false': 144, 'true': 56}, 'dev_counts': {'false': 529, 'true': 59}}
2023-06-01 20:20:48,556 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:20:50.057747: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:50.058187: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:20:57.834507: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:20:58,091 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:20:58,092 INFO     [orchestrator_api.py:376] new model id is 5ab0fb50-00f4-11ee-8161-e8ebd33a184c
2023-06-01 20:21:01,336 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1043 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:21:01,337 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/5ab0fb50-00f4-11ee-8161-e8ebd33a184c were not used when initializing TFBertForSequenceClassification: ['dropout_3077']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/5ab0fb50-00f4-11ee-8161-e8ebd33a184c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:21:02.802153: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:21:03,860 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:21:03,861 INFO     [orchestrator_api.py:376] new model id is 606a731e-00f4-11ee-9f77-e8ebd339fe3c
2023-06-01 20:21:04,028 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 2133 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:21:04,030 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/606a731e-00f4-11ee-9f77-e8ebd339fe3c were not used when initializing TFBertForSequenceClassification: ['dropout_2963']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/606a731e-00f4-11ee-9f77-e8ebd339fe3c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:21:05.738286: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:21:05,932 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:21:05.941096: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:21:06,011 INFO     [train_and_infer_api.py:122] finished running infer for 3519 values
2023-06-01 20:21:06,544 INFO     [disk_cache.py:28] saving 4596 items to disk cache took 0.519355297088623
2023-06-01 20:21:06,638 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: subjectivity_imbalanced_subjective_train and category: subjective.	runtime: 43.071807622909546	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:21:06,723 INFO     [train_and_infer_api.py:111] 3519 already in cache, running infer for 0 values (cache size 97593) in TrainAndInferHF
2023-06-01 20:21:06,858 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 97593) in TrainAndInferHF
2023-06-01 20:21:06,873 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:21:06,874 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:21:06,876 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:21:06,879 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:21:06,880 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:21:06,884 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:21:06,884 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:21:06,884 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:21:06,885 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:21:06,886 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:21:06,960 INFO     [train_and_dev_sets_selectors.py:27] using 450 for train using dataset subjectivity_imbalanced_subjective_train and 560 for dev using dataset subjectivity_imbalanced_subjective_dev
2023-06-01 20:21:06,961 INFO     [orchestrator_api.py:342] training a new model with {'false': 290, 'true': 160}
2023-06-01 20:21:06,962 INFO     [orchestrator_api.py:358] workspace imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING training a model for category 'subjective', model_metadata: {'train_counts': {'false': 290, 'true': 160}, 'dev_counts': {'false': 504, 'true': 56}}
2023-06-01 20:21:06,968 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:21:08.656322: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:21:08.656822: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:21:14,438 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:21:14,472 INFO     [train_and_infer_api.py:122] finished running infer for 1043 values
2023-06-01 20:21:14.483659: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:21:14,642 INFO     [disk_cache.py:28] saving 1039 items to disk cache took 0.15714406967163086
2023-06-01 20:21:14,712 INFO     [experiment_runner.py:179] Evaluation on dataset: cola_test, with AL: HARD_MINING, iteration: 12, repeat: 1, model (id: 5ab0fb50-00f4-11ee-8161-e8ebd33a184c) is: {'dataset': 'cola_test', 'category': '1', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 12, 'repeat id': 1, 'train positive count': 354, 'train negative count': 346, 'train total count': 700, 'BrierScore': 0.41610043289580656, 'LogLoss': 0.8647387198594877, 'confECE': 0.19869758626444015, 'cwECE': 0.17274960803876122, 'Acc': 76.51006711409396, 'MSE': nan, 'average_score': 0.9637982561284264, 'accuracy': 0.7651006711409396, 'precision': 0.7708095781071835, 'recall': 0.9388888888888889, 'f1': 0.8465873512836569, 'support': 720, 'tp': 676, 'fp': 201, 'tn': 122, 'fn': 44, 'diversity': 0.07535221182336357, 'representativeness': 0.18923777456930768}	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
2023-06-01 20:21:14,729 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 13, repeat num: 1	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
2023-06-01 20:21:14,896 INFO     [active_learning_api.py:59] Got 6877 unlabeled elements for active learning
2023-06-01 20:21:15,072 INFO     [train_and_infer_api.py:111] 15 already in cache, running infer for 6862 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:21:15,081 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/5ab0fb50-00f4-11ee-8161-e8ebd33a184c were not used when initializing TFBertForSequenceClassification: ['dropout_3077']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/5ab0fb50-00f4-11ee-8161-e8ebd33a184c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:21:17.375578: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:21:17.800865: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:21:21,977 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:21:21,979 INFO     [orchestrator_api.py:376] new model id is 6c2b6fa0-00f4-11ee-a89c-e8ebd335ce06
2023-06-01 20:21:22,122 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 500 values (cache size 52398) in TrainAndInferHF
2023-06-01 20:21:22,123 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/6c2b6fa0-00f4-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_1291']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/6c2b6fa0-00f4-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:21:23.325767: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:21:28,996 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:21:29,008 INFO     [train_and_infer_api.py:122] finished running infer for 500 values
2023-06-01 20:21:29,084 INFO     [disk_cache.py:28] saving 500 items to disk cache took 0.07257843017578125
2023-06-01 20:21:29,134 INFO     [experiment_runner.py:179] Evaluation on dataset: trec_test, with AL: RANDOM, iteration: 11, repeat: 1, model (id: 6c2b6fa0-00f4-11ee-a89c-e8ebd335ce06) is: {'dataset': 'trec_test', 'category': 'LOC', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 11, 'repeat id': 1, 'train positive count': 126, 'train negative count': 524, 'train total count': 650, 'BrierScore': 0.040057777552639166, 'LogLoss': 0.07453213576678738, 'confECE': 0.015789151485007487, 'cwECE': 0.008274180666077885, 'Acc': 97.2, 'MSE': nan, 'average_score': 0.9751238958835602, 'accuracy': 0.972, 'precision': 0.8941176470588236, 'recall': 0.9382716049382716, 'f1': 0.9156626506024096, 'support': 81, 'tp': 76, 'fp': 9, 'tn': 410, 'fn': 5, 'diversity': 0.41876816285318713, 'representativeness': 0.784247650941401}	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:21:29,155 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 12, repeat num: 1	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:21:29,286 INFO     [active_learning_api.py:59] Got 3968 unlabeled elements for active learning
2023-06-01 20:21:29,397 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: trec_train and category: LOC.	runtime: 0.2416088581085205	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:21:30,922 INFO     [train_and_infer_api.py:111] 8 already in cache, running infer for 4016 values (cache size 52898) in TrainAndInferHF
2023-06-01 20:21:30,926 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/6c2b6fa0-00f4-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_1291']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/6c2b6fa0-00f4-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:21:32.753876: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:21:33,002 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:21:33,036 INFO     [train_and_infer_api.py:122] finished running infer for 2133 values
2023-06-01 20:21:33,313 INFO     [disk_cache.py:28] saving 2133 items to disk cache took 0.24323272705078125
2023-06-01 20:21:33,385 INFO     [experiment_runner.py:179] Evaluation on dataset: polarity_test, with AL: HARD_MINING, iteration: 11, repeat: 1, model (id: 606a731e-00f4-11ee-9f77-e8ebd339fe3c) is: {'dataset': 'polarity_test', 'category': 'positive', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 11, 'repeat id': 1, 'train positive count': 293, 'train negative count': 357, 'train total count': 650, 'BrierScore': 0.3645654166863682, 'LogLoss': 0.8174369520566186, 'confECE': 0.1654622804315683, 'cwECE': 0.17770293640464885, 'Acc': 79.74683544303798, 'MSE': nan, 'average_score': 0.9629306354211967, 'accuracy': 0.7974683544303798, 'precision': 0.9300411522633745, 'recall': 0.6402266288951841, 'f1': 0.7583892617449665, 'support': 1059, 'tp': 678, 'fp': 51, 'tn': 1023, 'fn': 381, 'diversity': 0.0888597244130258, 'representativeness': 0.18740680500287546}	workspace: balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING
2023-06-01 20:21:33,389 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 12, repeat num: 1	workspace: balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING
2023-06-01 20:21:33,550 INFO     [active_learning_api.py:59] Got 6813 unlabeled elements for active learning
2023-06-01 20:21:33,826 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 6813 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:21:33,833 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/606a731e-00f4-11ee-9f77-e8ebd339fe3c were not used when initializing TFBertForSequenceClassification: ['dropout_2963']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/606a731e-00f4-11ee-9f77-e8ebd339fe3c.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:21:37.101328: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:21:37.173830: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:21:37,192 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:21:37,193 INFO     [orchestrator_api.py:376] new model id is 7013cb08-00f4-11ee-b1d5-e8ebd305f284
2023-06-01 20:21:37,395 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 2000 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:21:37,397 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:21:37,444 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:21:37,445 INFO     [orchestrator_api.py:376] new model id is 784f9eb4-00f4-11ee-b509-e8ebd329a958
2023-06-01 20:21:37,487 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1193 values (cache size 79653) in TrainAndInferHF
2023-06-01 20:21:37,489 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/7013cb08-00f4-11ee-b1d5-e8ebd305f284 were not used when initializing TFBertForSequenceClassification: ['dropout_3305']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/7013cb08-00f4-11ee-b1d5-e8ebd305f284.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/784f9eb4-00f4-11ee-b509-e8ebd329a958 were not used when initializing TFBertForSequenceClassification: ['dropout_1937']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/784f9eb4-00f4-11ee-b509-e8ebd329a958.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:21:38.890053: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:21:39.050490: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:21:52,193 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:21:52,213 INFO     [train_and_infer_api.py:122] finished running infer for 1193 values
2023-06-01 20:21:52,374 INFO     [disk_cache.py:28] saving 1193 items to disk cache took 0.1560380458831787
2023-06-01 20:21:52,412 INFO     [experiment_runner.py:179] Evaluation on dataset: polarity_imbalanced_positive_test, with AL: HARD_MINING, iteration: 2, repeat: 1, model (id: 784f9eb4-00f4-11ee-b509-e8ebd329a958) is: {'dataset': 'polarity_imbalanced_positive_test', 'category': 'positive', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 2, 'repeat id': 1, 'train positive count': 56, 'train negative count': 144, 'train total count': 200, 'BrierScore': 0.14527261820203344, 'LogLoss': 0.25478184241789975, 'confECE': 0.016934208458375347, 'cwECE': 0.028224572361339968, 'Acc': 90.61190276613578, 'MSE': nan, 'average_score': 0.9037349900575815, 'accuracy': 0.9061190276613579, 'precision': 0.5492957746478874, 'recall': 0.3277310924369748, 'f1': 0.4105263157894737, 'support': 119, 'tp': 39, 'fp': 32, 'tn': 1042, 'fn': 80, 'diversity': 0.08335016927934075, 'representativeness': 0.21017241618710902}	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING
2023-06-01 20:21:52,416 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 3, repeat num: 1	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING
2023-06-01 20:21:52,473 INFO     [active_learning_api.py:59] Got 3942 unlabeled elements for active learning
2023-06-01 20:21:52,629 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3942 values (cache size 80846) in TrainAndInferHF
2023-06-01 20:21:52,633 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/784f9eb4-00f4-11ee-b509-e8ebd329a958 were not used when initializing TFBertForSequenceClassification: ['dropout_1937']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/784f9eb4-00f4-11ee-b509-e8ebd329a958.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:21:53,998 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:21:53,999 INFO     [orchestrator_api.py:376] new model id is 7623dccc-00f4-11ee-b311-e8ebd329a920
2023-06-01 20:21:54,190 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3000 values (cache size 159057) in TrainAndInferHF
2023-06-01 20:21:54,193 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:21:55.085469: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/7623dccc-00f4-11ee-b311-e8ebd329a920 were not used when initializing TFBertForSequenceClassification: ['dropout_1063']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/7623dccc-00f4-11ee-b311-e8ebd329a920.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:21:56,581 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:21:56,582 INFO     [orchestrator_api.py:376] new model id is 8349040e-00f4-11ee-a5c3-e8ebd329a878
2023-06-01 20:21:56,670 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1077 values (cache size 97593) in TrainAndInferHF
2023-06-01 20:21:56,672 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:21:56.768816: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/8349040e-00f4-11ee-a5c3-e8ebd329a878 were not used when initializing TFBertForSequenceClassification: ['dropout_2507']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/8349040e-00f4-11ee-a5c3-e8ebd329a878.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:21:58.108954: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:22:02,087 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:22:02,116 INFO     [train_and_infer_api.py:122] finished running infer for 2000 values
2023-06-01 20:22:02,406 INFO     [disk_cache.py:28] saving 2000 items to disk cache took 0.26285743713378906
2023-06-01 20:22:02,472 INFO     [experiment_runner.py:179] Evaluation on dataset: subjectivity_test, with AL: HARD_MINING, iteration: 14, repeat: 1, model (id: 7013cb08-00f4-11ee-b1d5-e8ebd305f284) is: {'dataset': 'subjectivity_test', 'category': 'objective', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 14, 'repeat id': 1, 'train positive count': 386, 'train negative count': 414, 'train total count': 800, 'BrierScore': 0.06207628584054699, 'LogLoss': 0.11812364711422225, 'confECE': 0.019186323862467193, 'cwECE': 0.022464278079103683, 'Acc': 96.0, 'MSE': nan, 'average_score': 0.9791863240301609, 'accuracy': 0.96, 'precision': 0.9467336683417086, 'recall': 0.9721362229102167, 'f1': 0.9592668024439919, 'support': 969, 'tp': 942, 'fp': 53, 'tn': 978, 'fn': 27, 'diversity': 0.055757011021029035, 'representativeness': 0.1521545927400312}	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:22:02,475 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 15, repeat num: 1	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:22:02,655 INFO     [active_learning_api.py:59] Got 6200 unlabeled elements for active learning
2023-06-01 20:22:02,917 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 6200 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:22:02,923 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/7013cb08-00f4-11ee-b1d5-e8ebd305f284 were not used when initializing TFBertForSequenceClassification: ['dropout_3305']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/7013cb08-00f4-11ee-b1d5-e8ebd305f284.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:22:06.276773: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:22:10,187 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:22:10,207 INFO     [train_and_infer_api.py:122] finished running infer for 1077 values
2023-06-01 20:22:10,353 INFO     [disk_cache.py:28] saving 1077 items to disk cache took 0.14135169982910156
2023-06-01 20:22:10,397 INFO     [experiment_runner.py:179] Evaluation on dataset: subjectivity_imbalanced_subjective_test, with AL: HARD_MINING, iteration: 7, repeat: 1, model (id: 8349040e-00f4-11ee-a5c3-e8ebd329a878) is: {'dataset': 'subjectivity_imbalanced_subjective_test', 'category': 'subjective', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 7, 'repeat id': 1, 'train positive count': 160, 'train negative count': 290, 'train total count': 450, 'BrierScore': 0.04488977040687578, 'LogLoss': 0.08955581208278914, 'confECE': 0.009875741385979814, 'cwECE': 0.013861453813952967, 'Acc': 97.30733519034355, 'MSE': nan, 'average_score': 0.9814808413510867, 'accuracy': 0.9730733519034355, 'precision': 0.9247311827956989, 'recall': 0.7962962962962963, 'f1': 0.8557213930348259, 'support': 108, 'tp': 86, 'fp': 7, 'tn': 962, 'fn': 22, 'diversity': 0.07084636290890885, 'representativeness': 0.14747234647304208}	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:22:10,413 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 8, repeat num: 1	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:22:10,511 INFO     [active_learning_api.py:59] Got 3469 unlabeled elements for active learning
2023-06-01 20:22:10,659 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3469 values (cache size 98670) in TrainAndInferHF
2023-06-01 20:22:10,662 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/8349040e-00f4-11ee-a5c3-e8ebd329a878 were not used when initializing TFBertForSequenceClassification: ['dropout_2507']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/8349040e-00f4-11ee-a5c3-e8ebd329a878.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:22:12.945485: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:22:17,381 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:22:17,475 INFO     [train_and_infer_api.py:122] finished running infer for 4016 values
2023-06-01 20:22:18,013 INFO     [disk_cache.py:28] saving 4468 items to disk cache took 0.5213069915771484
2023-06-01 20:22:18,200 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 56866) in TrainAndInferHF
2023-06-01 20:22:18,226 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:18,233 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:18,234 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:18,241 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:18,242 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:18,248 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:18,248 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:18,249 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:18,249 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:18,250 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:18,336 INFO     [train_and_dev_sets_selectors.py:27] using 700 for train using dataset trec_train and 776 for dev using dataset trec_dev
2023-06-01 20:22:18,336 INFO     [orchestrator_api.py:342] training a new model with {'false': 567, 'true': 133}
2023-06-01 20:22:18,337 INFO     [orchestrator_api.py:358] workspace imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM training a model for category 'LOC', model_metadata: {'train_counts': {'false': 567, 'true': 133}, 'dev_counts': {'false': 658, 'true': 118}}
2023-06-01 20:22:18,345 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:22:19.966199: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:22:19.967253: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:22:30,279 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:22:30,333 INFO     [train_and_infer_api.py:122] finished running infer for 3000 values
2023-06-01 20:22:30,694 INFO     [disk_cache.py:28] saving 3000 items to disk cache took 0.34570932388305664
2023-06-01 20:22:33,363 INFO     [experiment_runner.py:179] Evaluation on dataset: ag_news_test, with AL: RANDOM, iteration: 9, repeat: 1, model (id: 7623dccc-00f4-11ee-b311-e8ebd329a920) is: {'dataset': 'ag_news_test', 'category': '1', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 9, 'repeat id': 1, 'train positive count': 125, 'train negative count': 425, 'train total count': 550, 'BrierScore': 0.11359364550994079, 'LogLoss': 0.22656338200245757, 'confECE': 0.036661631891437625, 'cwECE': 0.03657630070166973, 'Acc': 93.4, 'MSE': nan, 'average_score': 0.9706616319219271, 'accuracy': 0.934, 'precision': 0.8605898123324397, 'recall': 0.8722826086956522, 'f1': 0.8663967611336033, 'support': 736, 'tp': 642, 'fp': 104, 'tn': 2160, 'fn': 94, 'diversity': 0.24284822681108037, 'representativeness': 0.49344476812423305}	workspace: balanced_ag_news_calib_nseed_100_step_50_nactive_15_MC_10-ag_news-1-HFBERT-1-RANDOM
2023-06-01 20:22:33,403 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 10, repeat num: 1	workspace: balanced_ag_news_calib_nseed_100_step_50_nactive_15_MC_10-ag_news-1-HFBERT-1-RANDOM
2023-06-01 20:22:33,601 INFO     [active_learning_api.py:59] Got 14432 unlabeled elements for active learning
2023-06-01 20:22:33,712 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: ag_news_train and category: 1.	runtime: 0.30786585807800293	workspace: balanced_ag_news_calib_nseed_100_step_50_nactive_15_MC_10-ag_news-1-HFBERT-1-RANDOM
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:22:34,092 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:22:34,226 INFO     [train_and_infer_api.py:122] finished running infer for 6862 values
2023-06-01 20:22:34,381 INFO     [train_and_infer_api.py:111] 9 already in cache, running infer for 14441 values (cache size 162057) in TrainAndInferHF
2023-06-01 20:22:34,396 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:22:35,234 INFO     [disk_cache.py:28] saving 7901 items to disk cache took 0.9402337074279785
2023-06-01 20:22:35,403 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: cola_train and category: 1.	runtime: 80.67389035224915	workspace: balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/7623dccc-00f4-11ee-b311-e8ebd329a920 were not used when initializing TFBertForSequenceClassification: ['dropout_1063']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/7623dccc-00f4-11ee-b311-e8ebd329a920.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:22:35,561 INFO     [train_and_infer_api.py:111] 6892 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:22:35,818 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:22:35,844 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:35,846 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:35,847 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:35,861 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:35,862 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:35,873 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:35,873 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:35,874 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:35,875 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:35,875 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:35,981 INFO     [train_and_dev_sets_selectors.py:27] using 750 for train using dataset cola_train and 959 for dev using dataset cola_dev
2023-06-01 20:22:35,982 INFO     [orchestrator_api.py:342] training a new model with {'false': 369, 'true': 381}
2023-06-01 20:22:35,982 INFO     [orchestrator_api.py:358] workspace balanced_cola_calib_nseed_100_step_50_nactive_15_MC_10-cola-1-HFBERT-1-HARD_MINING training a model for category '1', model_metadata: {'train_counts': {'false': 369, 'true': 381}, 'dev_counts': {'true': 673, 'false': 286}}
2023-06-01 20:22:35,992 INFO     [train_and_infer_hf.py:96] Training hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:22:36,019 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:22:36,307 INFO     [train_and_infer_api.py:122] finished running infer for 14641 values
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:22:37.700494: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:22:37.701611: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:22:38,365 INFO     [disk_cache.py:28] saving 17635 items to disk cache took 2.0090019702911377
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:22:38,750 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:22:38,829 INFO     [train_and_infer_api.py:122] finished running infer for 3942 values
2023-06-01 20:22:38,830 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 106560) in TrainAndInferHF
2023-06-01 20:22:38,846 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:38,851 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:38,851 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:38,864 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:38,865 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:38,879 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:38,880 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:38,880 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:38,881 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:38,882 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:39,058 INFO     [train_and_dev_sets_selectors.py:27] using 400 for train using dataset wiki_attack_train and 2998 for dev using dataset wiki_attack_dev
2023-06-01 20:22:39,058 INFO     [orchestrator_api.py:342] training a new model with {'false': 319, 'true': 81}
2023-06-01 20:22:39,059 INFO     [orchestrator_api.py:358] workspace imbalanced_wiki_attack_calib_nseed_100_step_50_nactive_15_MC_10-wiki_attack-True-HFBERT-1-RANDOM training a model for category 'True', model_metadata: {'train_counts': {'false': 319, 'true': 81}, 'dev_counts': {'false': 2676, 'true': 322}}
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:22:39,355 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:22:39,439 INFO     [disk_cache.py:28] saving 5135 items to disk cache took 0.5971624851226807
2023-06-01 20:22:39,494 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: polarity_imbalanced_positive_train and category: positive.	runtime: 47.07750964164734	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING
2023-06-01 20:22:39,564 INFO     [train_and_infer_api.py:111] 3942 already in cache, running infer for 0 values (cache size 84788) in TrainAndInferHF
2023-06-01 20:22:39,572 INFO     [train_and_infer_api.py:122] finished running infer for 12063 values
2023-06-01 20:22:39,685 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 84788) in TrainAndInferHF
2023-06-01 20:22:39,693 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:39,695 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:39,696 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:39,699 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:39,700 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:39,703 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:39,704 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:39,704 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:39,705 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:39,706 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:39,775 INFO     [train_and_dev_sets_selectors.py:27] using 250 for train using dataset polarity_imbalanced_positive_train and 588 for dev using dataset polarity_imbalanced_positive_dev
2023-06-01 20:22:39,776 INFO     [orchestrator_api.py:342] training a new model with {'false': 172, 'true': 78}
2023-06-01 20:22:39,777 INFO     [orchestrator_api.py:358] workspace imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING training a model for category 'positive', model_metadata: {'train_counts': {'false': 172, 'true': 78}, 'dev_counts': {'false': 529, 'true': 59}}
2023-06-01 20:22:39,782 INFO     [train_and_infer_hf.py:96] Training hf model...
2023-06-01 20:22:40,668 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:22:41,240 INFO     [disk_cache.py:28] saving 14560 items to disk cache took 1.6300034523010254
2023-06-01 20:22:41.362953: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:22:41.363423: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:22:41,627 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 132840) in TrainAndInferHF
2023-06-01 20:22:41,649 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:41,653 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:41,654 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:41,664 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:41,665 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:41,675 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:41,676 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:41,676 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:41,678 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:41,679 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:22:41,838 INFO     [train_and_dev_sets_selectors.py:27] using 550 for train using dataset ag_news_imbalanced_1_train and 2455 for dev using dataset ag_news_imbalanced_1_dev
2023-06-01 20:22:41,839 INFO     [orchestrator_api.py:342] training a new model with {'false': 447, 'true': 103}
2023-06-01 20:22:41,841 INFO     [orchestrator_api.py:358] workspace imbalanced_ag_news_imbalanced_1_calib_nseed_100_step_50_nactive_15_MC_10-ag_news_imbalanced_1-1-HFBERT-1-RANDOM training a model for category '1', model_metadata: {'train_counts': {'false': 447, 'true': 103}, 'dev_counts': {'false': 2209, 'true': 246}}
2023-06-01 20:22:41,856 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:22:43.026365: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:22:44.554036: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:22:44.554571: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:22:45.562318: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:22:45.563014: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:22:51,454 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:22:51,514 INFO     [train_and_infer_api.py:122] finished running infer for 3469 values
2023-06-01 20:22:51.664241: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:22:52,065 INFO     [disk_cache.py:28] saving 4546 items to disk cache took 0.5230710506439209
2023-06-01 20:22:52,180 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: subjectivity_imbalanced_subjective_train and category: subjective.	runtime: 41.76641058921814	workspace: imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING
2023-06-01 20:22:52,265 INFO     [train_and_infer_api.py:111] 3469 already in cache, running infer for 0 values (cache size 102139) in TrainAndInferHF
2023-06-01 20:22:52,399 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 102139) in TrainAndInferHF
2023-06-01 20:22:52,416 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:52,417 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:52,418 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:52,421 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:52,421 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:52,425 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:52,425 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:22:52,426 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:52,427 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:22:52,428 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:22:52,523 INFO     [train_and_dev_sets_selectors.py:27] using 500 for train using dataset subjectivity_imbalanced_subjective_train and 560 for dev using dataset subjectivity_imbalanced_subjective_dev
2023-06-01 20:22:52,524 INFO     [orchestrator_api.py:342] training a new model with {'false': 314, 'true': 186}
2023-06-01 20:22:52,526 INFO     [orchestrator_api.py:358] workspace imbalanced_subjectivity_imbalanced_subjective_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity_imbalanced_subjective-subjective-HFBERT-1-HARD_MINING training a model for category 'subjective', model_metadata: {'train_counts': {'false': 314, 'true': 186}, 'dev_counts': {'false': 504, 'true': 56}}
2023-06-01 20:22:52,531 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:22:54.166918: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:22:54.167367: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:23:06.034341: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:23:07,453 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:23:07,566 INFO     [train_and_infer_api.py:122] finished running infer for 6813 values
2023-06-01 20:23:08.530771: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:23:08,690 INFO     [disk_cache.py:28] saving 8946 items to disk cache took 1.0646727085113525
2023-06-01 20:23:08,872 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: polarity_train and category: positive.	runtime: 95.48207759857178	workspace: balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING
2023-06-01 20:23:11,678 INFO     [train_and_infer_api.py:111] 6813 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:23:11,925 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:23:11,950 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:11,952 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:23:11,952 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:11,958 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:23:11,959 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:11,963 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:11,963 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:23:11,964 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:11,965 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:23:11,965 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:12,070 INFO     [train_and_dev_sets_selectors.py:27] using 700 for train using dataset polarity_train and 1066 for dev using dataset polarity_dev
2023-06-01 20:23:12,070 INFO     [orchestrator_api.py:342] training a new model with {'false': 365, 'true': 335}
2023-06-01 20:23:12,071 INFO     [orchestrator_api.py:358] workspace balanced_polarity_calib_nseed_100_step_50_nactive_15_MC_10-polarity-positive-HFBERT-1-HARD_MINING training a model for category 'positive', model_metadata: {'train_counts': {'false': 365, 'true': 335}, 'dev_counts': {'true': 537, 'false': 529}}
2023-06-01 20:23:12,080 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:23:13.152980: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:23:13.772065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:23:13.773154: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:23:14.110513: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:23:16,461 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:23:16,462 INFO     [orchestrator_api.py:376] new model id is add4962a-00f4-11ee-a89c-e8ebd335ce06
2023-06-01 20:23:16,605 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 500 values (cache size 56866) in TrainAndInferHF
2023-06-01 20:23:16,606 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/add4962a-00f4-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_1405']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/add4962a-00f4-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:23:17.587949: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:23:17,795 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:23:17,897 INFO     [train_and_infer_api.py:122] finished running infer for 6200 values
2023-06-01 20:23:18,850 INFO     [disk_cache.py:28] saving 8200 items to disk cache took 0.9012579917907715
2023-06-01 20:23:19,041 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: HARD_MINING for dataset: subjectivity_train and category: objective.	runtime: 76.56570386886597	workspace: balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING
2023-06-01 20:23:19,209 INFO     [train_and_infer_api.py:111] 6200 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:23:19,452 INFO     [train_and_infer_api.py:111] 50 already in cache, running infer for 0 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:23:19,482 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:19,484 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:23:19,485 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:19,491 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:23:19,492 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:19,498 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:19,499 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.fit: running accelerated version on CPU
2023-06-01 20:23:19,500 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:19,500 INFO     [_device_offload.py:161] sklearn.neighbors.NearestNeighbors.kneighbors: running accelerated version on CPU
2023-06-01 20:23:19,501 INFO     [_utils.py:208] sklearn.utils.validation._assert_all_finite: running accelerated version on CPU
2023-06-01 20:23:19,607 INFO     [train_and_dev_sets_selectors.py:27] using 850 for train using dataset subjectivity_train and 1000 for dev using dataset subjectivity_dev
2023-06-01 20:23:19,607 INFO     [orchestrator_api.py:342] training a new model with {'false': 452, 'true': 398}
2023-06-01 20:23:19,609 INFO     [orchestrator_api.py:358] workspace balanced_subjectivity_calib_nseed_100_step_50_nactive_15_MC_10-subjectivity-objective-HFBERT-1-HARD_MINING training a model for category 'objective', model_metadata: {'train_counts': {'false': 452, 'true': 398}, 'dev_counts': {'true': 504, 'false': 496}}
2023-06-01 20:23:19,618 INFO     [train_and_infer_hf.py:96] Training hf model...
All model checkpoint layers were used when initializing TFBertForSequenceClassification.

Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2023-06-01 20:23:21.319464: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:23:21.319971: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:23:23,168 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:23:23,178 INFO     [train_and_infer_api.py:122] finished running infer for 500 values
2023-06-01 20:23:23.226198: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:23:23,268 INFO     [disk_cache.py:28] saving 500 items to disk cache took 0.0869896411895752
2023-06-01 20:23:23,325 INFO     [experiment_runner.py:179] Evaluation on dataset: trec_test, with AL: RANDOM, iteration: 12, repeat: 1, model (id: add4962a-00f4-11ee-a89c-e8ebd335ce06) is: {'dataset': 'trec_test', 'category': 'LOC', 'model': 'HFBERT', 'AL': 'RANDOM', 'iteration number': 12, 'repeat id': 1, 'train positive count': 133, 'train negative count': 567, 'train total count': 700, 'BrierScore': 0.05665538505369879, 'LogLoss': 0.13006947366774424, 'confECE': 0.022694514623293596, 'cwECE': 0.014520592050394042, 'Acc': 96.8, 'MSE': nan, 'average_score': 0.9885720254182816, 'accuracy': 0.968, 'precision': 0.9452054794520548, 'recall': 0.8518518518518519, 'f1': 0.8961038961038961, 'support': 81, 'tp': 69, 'fp': 4, 'tn': 415, 'fn': 12, 'diversity': 0.3573378622081395, 'representativeness': 0.6221177232816609}	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:23:23,329 INFO     [experiment_runner.py:108] Run AL strategy: RANDOM, iteration num: 13, repeat num: 1	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
2023-06-01 20:23:23,471 INFO     [active_learning_api.py:59] Got 3918 unlabeled elements for active learning
2023-06-01 20:23:23,588 INFO     [experiment_runner.py:190] 50 instances suggested by active learning strategy: RANDOM for dataset: trec_train and category: LOC.	runtime: 0.25905823707580566	workspace: imbalanced_trec_calib_nseed_100_step_50_nactive_15_MC_10-trec-LOC-HFBERT-1-RANDOM
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/keras/engine/training.py:2448: UserWarning: Metric SparseF1 implements a `reset_states()` method; rename it to `reset_state()` (without the final "s"). The name `reset_states()` has been deprecated to improve API consistency.
  m.reset_state()
2023-06-01 20:23:25,132 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:23:25,133 INFO     [orchestrator_api.py:376] new model id is ba9b789c-00f4-11ee-b509-e8ebd329a958
2023-06-01 20:23:25,177 INFO     [train_and_infer_api.py:111] 8 already in cache, running infer for 3966 values (cache size 57366) in TrainAndInferHF
2023-06-01 20:23:25,181 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
2023-06-01 20:23:25,185 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1193 values (cache size 84788) in TrainAndInferHF
2023-06-01 20:23:25,186 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/add4962a-00f4-11ee-a89c-e8ebd335ce06 were not used when initializing TFBertForSequenceClassification: ['dropout_1405']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/add4962a-00f4-11ee-a89c-e8ebd335ce06.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
Some layers from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/ba9b789c-00f4-11ee-b509-e8ebd329a958 were not used when initializing TFBertForSequenceClassification: ['dropout_2051']
- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /scratch/pkadambi/low-resource-text-classification-framework/lrtc_lib/output/models/transformers/ba9b789c-00f4-11ee-b509-e8ebd329a958.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.
2023-06-01 20:23:26.846363: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
2023-06-01 20:23:26.888563: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
	 [[{{node Placeholder/_0}}]]
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:23:34,598 INFO     [train_and_infer_hf.py:155] Training done
2023-06-01 20:23:34,599 INFO     [orchestrator_api.py:376] new model id is b858fbb8-00f4-11ee-8161-e8ebd33a184c
2023-06-01 20:23:39,648 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 1043 values (cache size 200000) in TrainAndInferHF
2023-06-01 20:23:39,649 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
/home/pkadambi/.conda/envs/al_ibm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
2023-06-01 20:23:40,224 INFO     [train_and_infer_hf.py:184] Infer hf model done
2023-06-01 20:23:40,246 INFO     [train_and_infer_api.py:122] finished running infer for 1193 values
2023-06-01 20:23:40,430 INFO     [disk_cache.py:28] saving 1193 items to disk cache took 0.17904305458068848
2023-06-01 20:23:40,470 INFO     [experiment_runner.py:179] Evaluation on dataset: polarity_imbalanced_positive_test, with AL: HARD_MINING, iteration: 3, repeat: 1, model (id: ba9b789c-00f4-11ee-b509-e8ebd329a958) is: {'dataset': 'polarity_imbalanced_positive_test', 'category': 'positive', 'model': 'HFBERT', 'AL': 'HARD_MINING', 'iteration number': 3, 'repeat id': 1, 'train positive count': 78, 'train negative count': 172, 'train total count': 250, 'BrierScore': 0.19063929353068582, 'LogLoss': 0.31222121184145096, 'confECE': 0.05655253710484319, 'cwECE': 0.07432118580151233, 'Acc': 86.33696563285834, 'MSE': nan, 'average_score': 0.9199221942071043, 'accuracy': 0.8633696563285834, 'precision': 0.3735632183908046, 'recall': 0.5462184873949579, 'f1': 0.4436860068259386, 'support': 119, 'tp': 65, 'fp': 109, 'tn': 965, 'fn': 54, 'diversity': 0.06702645745465262, 'representativeness': 0.23577798386335239}	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING
2023-06-01 20:23:40,473 INFO     [experiment_runner.py:108] Run AL strategy: HARD_MINING, iteration num: 4, repeat num: 1	workspace: imbalanced_polarity_imbalanced_positive_calib_nseed_100_step_50_nactive_15_MC_10-polarity_imbalanced_positive-positive-HFBERT-1-HARD_MINING
2023-06-01 20:23:40,539 INFO     [active_learning_api.py:59] Got 3892 unlabeled elements for active learning
2023-06-01 20:23:40,691 INFO     [train_and_infer_api.py:111] 0 already in cache, running infer for 3892 values (cache size 85981) in TrainAndInferHF
2023-06-01 20:23:40,694 INFO     [train_and_infer_hf.py:160] Inferring with hf model...
